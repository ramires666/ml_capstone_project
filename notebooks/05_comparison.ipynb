{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50426e04",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "05 - MODEL COMPARISON\n",
    "==============================================================================\n",
    "\n",
    "PURPOSE OF THIS NOTEBOOK:\n",
    "-------------------------\n",
    "Compare the XGBoost baseline and CNN-LSTM advanced models to:\n",
    "1. Determine which model performs better\n",
    "2. Understand strengths/weaknesses of each\n",
    "3. Visualize predictions compared to actual labels\n",
    "4. Make recommendations for production deployment\n",
    "\n",
    "WHY COMPARE MODELS?\n",
    "-------------------\n",
    "- No single model is best for all situations\n",
    "- Different models may excel at different classes\n",
    "- Ensemble of models often outperforms individual models\n",
    "- Understanding trade-offs helps with deployment decisions\n",
    "\n",
    "WHAT YOU'LL LEARN:\n",
    "------------------\n",
    "- How to evaluate models fairly on the same test set\n",
    "- Per-class performance analysis (precision, recall, F1)\n",
    "- Visual comparison of confusion matrices\n",
    "- How to make data-driven model selection decisions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac3081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99e28a1",
   "metadata": {},
   "source": [
    "# Model Comparison: XGBoost vs CNN-LSTM\n",
    "\n",
    "**Goal**: Determine which model to deploy for production.\n",
    "\n",
    "**Key Questions**:\n",
    "1. Which model has higher overall accuracy?\n",
    "2. Which model is better at predicting specific classes?\n",
    "3. Are the differences statistically significant?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aad28cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# ==============================================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Works both as script (__file__ exists) and in Jupyter (use cwd)\n",
    "import os\n",
    "try:\n",
    "    project_root = Path(__file__).resolve().parent.parent\n",
    "except NameError:\n",
    "    cwd = Path(os.getcwd())\n",
    "    project_root = cwd.parent if cwd.name == 'notebooks' else cwd\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f76c8841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# IMPORT PROJECT MODULES\n",
    "# ==============================================================================\n",
    "\n",
    "import importlib\n",
    "import src.data.loader\n",
    "import src.labeling.oracle\n",
    "import src.features.builder\n",
    "import src.features.indicators\n",
    "import src.models.xgb\n",
    "import src.models.cnn_lstm\n",
    "try:\n",
    "    import src.models.tcn_attention\n",
    "    importlib.reload(src.models.tcn_attention)\n",
    "    from src.models.tcn_attention import TCNAttentionModel\n",
    "    tcn_module_available = True\n",
    "except ImportError:\n",
    "    tcn_module_available = False\n",
    "\n",
    "importlib.reload(src.data.loader)\n",
    "importlib.reload(src.labeling.oracle)\n",
    "importlib.reload(src.features.builder)\n",
    "importlib.reload(src.features.indicators)\n",
    "importlib.reload(src.models.xgb)\n",
    "importlib.reload(src.models.cnn_lstm)\n",
    "\n",
    "from src.data.loader import load_and_merge_data, split_data_by_time\n",
    "from src.labeling.oracle import create_oracle_labels\n",
    "from src.features.builder import prepare_features\n",
    "from src.features.indicators import get_indicator_columns\n",
    "from src.models.xgb import XGBBaseline\n",
    "from src.models.cnn_lstm import CNNLSTMModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b256534",
   "metadata": {},
   "source": [
    "## 1. Load Models and Test Data\n",
    "\n",
    "**CRITICAL**: Both models must be evaluated on the SAME test set.\n",
    "This ensures a fair comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bf8eb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "๐ CONFIGURATION\n",
      "============================================================\n",
      "Horizon: 1 bar(s)\n",
      "Model directory: models_artifacts/\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "HORIZON = 1                    # Prediction horizon (must match trained models)\n",
    "MODEL_DIR = 'models_artifacts'  # Directory with saved models\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"๐ CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Horizon: {HORIZON} bar(s)\")\n",
    "print(f\"Model directory: {MODEL_DIR}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b78a1ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "๐ฅ LOADING TEST DATA\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "๐ฅ LOADING DATA\n",
      "============================================================\n",
      "\n",
      "โ Loading pre-merged dataset: /mnt/c/_PYTH/projects/capstone_project/data/processed/all_merged.parquet\n",
      "โฐ Filtered until: 2025-12-31\n",
      "\n",
      "๐ Data Summary:\n",
      "   Rows: 70,081\n",
      "   Columns: 21\n",
      "   Date range: 2024-01-01 00:00:00+00:00 to 2025-12-31 00:00:00+00:00\n",
      "\n",
      "============================================================\n",
      "๐ง PREPARING FEATURES\n",
      "============================================================\n",
      "Groups to add: ['momentum', 'overlap', 'trend', 'volatility', 'volume', 'statistics', 'candle']\n",
      "\n",
      "๐ Adding indicators...\n",
      "  -> Processing group: momentum\n",
      "     Added 14 features: TSIs_13_25_13, RSI_14, MOM_10, TSI_13_25_13, STOCHh_14_3_3...\n",
      "  -> Processing group: overlap\n",
      "[!] VWAP requires an ordered DatetimeIndex.\n",
      "     Added 7 features: EMA_100, EMA_50, HMA_9, TEMA_9, EMA_20...\n",
      "  -> Processing group: trend\n",
      "     Added 17 features: ADXR_14_2, MACD_12_26_9, DMN_14, TRIXs_30_9, AROOND_14...\n",
      "  -> Processing group: volatility\n",
      "       BBL_20_2.0_2.0  BBM_20_2.0_2.0  BBU_20_2.0_2.0  BBB_20_2.0_2.0  \\\n",
      "0                 NaN             NaN             NaN             NaN   \n",
      "1                 NaN             NaN             NaN             NaN   \n",
      "2                 NaN             NaN             NaN             NaN   \n",
      "3                 NaN             NaN             NaN             NaN   \n",
      "4                 NaN             NaN             NaN             NaN   \n",
      "...               ...             ...             ...             ...   \n",
      "70076    87906.208343       88214.555    88522.901657        0.699083   \n",
      "70077    87889.395082       88224.055    88558.714918        0.758659   \n",
      "70078    87878.371545       88232.860    88587.348455        0.803529   \n",
      "70079    87874.507609       88242.645    88610.782391        0.834375   \n",
      "70080    87902.480057       88258.450    88614.419943        0.806654   \n",
      "\n",
      "       BBP_20_2.0_2.0  \n",
      "0                 NaN  \n",
      "1                 NaN  \n",
      "2                 NaN  \n",
      "3                 NaN  \n",
      "4                 NaN  \n",
      "...               ...  \n",
      "70076        0.811897  \n",
      "70077        0.950823  \n",
      "70078        0.877925  \n",
      "70079        0.788826  \n",
      "70080        0.617917  \n",
      "\n",
      "[70081 rows x 5 columns]\n",
      "          KCLe_20_2     KCBe_20_2     KCUe_20_2\n",
      "0               NaN           NaN           NaN\n",
      "1               NaN           NaN           NaN\n",
      "2               NaN           NaN           NaN\n",
      "3               NaN           NaN           NaN\n",
      "4               NaN           NaN           NaN\n",
      "...             ...           ...           ...\n",
      "70076  87899.333398  88275.506712  88651.680026\n",
      "70077  87928.711170  88299.344168  88669.977167\n",
      "70078  87971.119630  88318.530438  88665.941246\n",
      "70079  88002.317760  88331.556111  88660.794461\n",
      "70080  88005.392259  88332.588862  88659.785465\n",
      "\n",
      "[70081 rows x 3 columns]\n",
      "       DCL_20_20  DCM_20_20  DCU_20_20\n",
      "0            NaN        NaN        NaN\n",
      "1            NaN        NaN        NaN\n",
      "2            NaN        NaN        NaN\n",
      "3            NaN        NaN        NaN\n",
      "4            NaN        NaN        NaN\n",
      "...          ...        ...        ...\n",
      "70076    87833.2    88151.6    88470.0\n",
      "70077    87833.2    88183.1    88533.0\n",
      "70078    87833.2    88183.1    88533.0\n",
      "70079    87833.2    88183.1    88533.0\n",
      "70080    87833.2    88183.1    88533.0\n",
      "\n",
      "[70081 rows x 3 columns]\n",
      "     Added 5 features: BBP_20_2.0_2.0, NATR_14, ATRr_14, UI_14, BBB_20_2.0_2.0\n",
      "  -> Processing group: volume\n",
      "     Added 8 features: CMF_20, EOM_14_100000000, MFI_14, AD, OBV...\n",
      "  -> Processing group: statistics\n",
      "     Added 6 features: ENTP_30, MAD_30, SKEW_30, KURT_30, VAR_30...\n",
      "  -> Processing group: candle\n",
      "     Added 0 features: \n",
      "  -> Orderflow features found: 8\n",
      "     bid_vol, ask_vol, max_bid_vol, max_ask_vol, avg_bid_vol, avg_ask_vol, delta_vol, cvd\n",
      "\n",
      "โฐ Shifting target by -1 for prediction...\n",
      "\n",
      "๐ Top NaN columns:\n",
      "   PVIe_255: 254 NaN\n",
      "   SMA_200: 199 NaN\n",
      "   EMA_100: 99 NaN\n",
      "   ENTP_30: 58 NaN\n",
      "   EMA_50: 49 NaN\n",
      "\n",
      "๐ Warmup period: 254 rows, target shift: 1 row\n",
      "   Filling 17 remaining NaN with 0\n",
      "\n",
      "๐งน Dropped 255 rows with NaN (69,826 remaining)\n",
      "\n",
      "๐ Feature Summary:\n",
      "   Total rows: 69,826\n",
      "   Total columns: 81\n",
      "   Indicator groups:\n",
      "      momentum: 14 features\n",
      "      overlap: 7 features\n",
      "      trend: 17 features\n",
      "      volatility: 5 features\n",
      "      volume: 8 features\n",
      "      statistics: 6 features\n",
      "      candle: 0 features\n",
      "      orderflow: 8 features\n",
      "      other: 2 features\n",
      "\n",
      "๐ Data Split:\n",
      "   Train: 46,947 rows (2024-01-03 15:30:00+00:00 to 2025-05-06 16:00:00+00:00)\n",
      "   Val:   5,216 rows (2025-05-06 16:15:00+00:00 to 2025-06-30 00:00:00+00:00)\n",
      "   Test:  17,568 rows (2025-07-01 00:00:00+00:00 to 2025-12-30 23:45:00+00:00)\n",
      "\n",
      "โ Test set loaded: 17,568 samples ร 67 features\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# LOAD TEST DATA\n",
    "# ==============================================================================\n",
    "#\n",
    "# We recreate the exact same test data used during training.\n",
    "# This ensures fair comparison.\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"๐ฅ LOADING TEST DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df = load_and_merge_data(end_date='2025-12-31')\n",
    "df = create_oracle_labels(df, sigma=3, threshold=0.0002)\n",
    "df_features, _ = prepare_features(df, horizon=HORIZON)\n",
    "\n",
    "# Split to get test set\n",
    "_, _, test_df = split_data_by_time(\n",
    "    df_features,\n",
    "    train_end=\"2025-06-30\",\n",
    "    test_start=\"2025-07-01\"\n",
    ")\n",
    "\n",
    "# Get feature columns\n",
    "feature_cols = get_indicator_columns(\n",
    "    df_features, \n",
    "    exclude_cols=['time', 'target', 'smoothed_close', 'smooth_slope']\n",
    ")\n",
    "feature_cols = [c for c in feature_cols if c in test_df.columns]\n",
    "\n",
    "# Extract test arrays\n",
    "X_test = test_df[feature_cols].values\n",
    "y_test = test_df['target'].values.astype(int)\n",
    "\n",
    "# Clean inf/nan values (some indicators like EOM produce inf from division by zero)\n",
    "X_test = np.nan_to_num(X_test, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "print(f\"\\nโ Test set loaded: {X_test.shape[0]:,} samples ร {X_test.shape[1]} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9f54771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "๐ LOADING TRAINED MODELS\n",
      "============================================================\n",
      "โ Model loaded from models_artifacts\n",
      "โ XGBoost model loaded: models_artifacts/xgb_baseline_h1_model.joblib\n",
      "๐ GPU detected: /physical_device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1769586513.105338  132413 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10537 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "โ Model loaded from models_artifacts\n",
      "โ CNN-LSTM model loaded: models_artifacts/cnn_lstm_h1_model.keras\n",
      "๐ GPU detected: /physical_device:GPU:0\n",
      "โ Model loaded from models_artifacts\n",
      "โ TCN-Attention model loaded: models_artifacts/tcn_attention_h1_model.keras\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# LOAD TRAINED MODELS\n",
    "# ==============================================================================\n",
    "#\n",
    "# We load both models from disk.\n",
    "# If a model doesn't exist, we skip it in the comparison.\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"๐ LOADING TRAINED MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load XGBoost model\n",
    "try:\n",
    "    xgb_model = XGBBaseline.load(MODEL_DIR, name=f'xgb_baseline_h{HORIZON}')\n",
    "    xgb_available = True\n",
    "    print(f\"โ XGBoost model loaded: {MODEL_DIR}/xgb_baseline_h{HORIZON}_model.joblib\")\n",
    "except Exception as e:\n",
    "    print(f\"โ๏ธ XGBoost model not found: {e}\")\n",
    "    xgb_available = False\n",
    "\n",
    "# Load CNN-LSTM model\n",
    "try:\n",
    "    cnn_model = CNNLSTMModel.load(MODEL_DIR, name=f'cnn_lstm_h{HORIZON}', device='cuda')\n",
    "    cnn_available = True\n",
    "    print(f\"โ CNN-LSTM model loaded: {MODEL_DIR}/cnn_lstm_h{HORIZON}_model.keras\")\n",
    "except Exception as e:\n",
    "    print(f\"โ๏ธ CNN-LSTM model not found: {e}\")\n",
    "    cnn_available = False\n",
    "\n",
    "# Load TCN-Attention model (if available)\n",
    "tcn_available = False\n",
    "if tcn_module_available:\n",
    "    try:\n",
    "        tcn_model = TCNAttentionModel.load(MODEL_DIR, name=f'tcn_attention_h{HORIZON}', device='cuda')\n",
    "        tcn_available = True\n",
    "        print(f\"โ TCN-Attention model loaded: {MODEL_DIR}/tcn_attention_h{HORIZON}_model.keras\")\n",
    "    except Exception as e:\n",
    "        print(f\"โ๏ธ TCN-Attention model not found: {e}\")\n",
    "\n",
    "if not xgb_available and not cnn_available:\n",
    "    print(\"\\nโ No models found! Run notebooks 02 and 03 first to train models.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ba4b3c",
   "metadata": {},
   "source": [
    "## 2. Evaluate Models\n",
    "\n",
    "**Metrics Explained:**\n",
    "- **Accuracy**: Overall percentage of correct predictions\n",
    "- **F1 Weighted**: Weighted average F1, good for imbalanced classes\n",
    "- **F1 Macro**: Simple average F1, treats all classes equally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc2ac5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "๐ EVALUATING MODELS\n",
      "============================================================\n",
      "\n",
      "Evaluating XGBoost...\n",
      "  Accuracy: 0.5745\n",
      "\n",
      "Evaluating CNN-LSTM...\n",
      "  Accuracy: 0.5256\n",
      "\n",
      "Evaluating TCN-Attention...\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# EVALUATE BOTH MODELS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"๐ EVALUATING MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Evaluate XGBoost\n",
    "if xgb_available:\n",
    "    print(\"\\nEvaluating XGBoost...\")\n",
    "    xgb_metrics = xgb_model.evaluate(X_test, y_test)\n",
    "    xgb_preds = xgb_model.predict(X_test)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': 'XGBoost',\n",
    "        'Accuracy': xgb_metrics['accuracy'],\n",
    "        'F1 Weighted': xgb_metrics['f1_weighted'],\n",
    "        'F1 Macro': xgb_metrics['f1_macro']\n",
    "    })\n",
    "    print(f\"  Accuracy: {xgb_metrics['accuracy']:.4f}\")\n",
    "\n",
    "# Evaluate CNN-LSTM\n",
    "if cnn_available:\n",
    "    print(\"\\nEvaluating CNN-LSTM...\")\n",
    "    cnn_metrics = cnn_model.evaluate(X_test, y_test)\n",
    "    cnn_preds = cnn_model.predict(X_test)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': 'CNN-LSTM',\n",
    "        'Accuracy': cnn_metrics['accuracy'],\n",
    "        'F1 Weighted': cnn_metrics['f1_weighted'],\n",
    "        'F1 Macro': cnn_metrics['f1_macro']\n",
    "    })\n",
    "    print(f\"  Accuracy: {cnn_metrics['accuracy']:.4f}\")\n",
    "\n",
    "# Evaluate TCN-Attention\n",
    "if tcn_available:\n",
    "    print(\"\\nEvaluating TCN-Attention...\")\n",
    "    tcn_metrics = tcn_model.evaluate(X_test, y_test)\n",
    "    tcn_preds = tcn_model.predict(X_test)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': 'TCN-Attention',\n",
    "        'Accuracy': tcn_metrics['accuracy'],\n",
    "        'F1 Weighted': tcn_metrics['f1_weighted'],\n",
    "        'F1 Macro': tcn_metrics['f1_macro']\n",
    "    })\n",
    "    print(f\"  Accuracy: {tcn_metrics['accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaa077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DISPLAY COMPARISON TABLE\n",
    "# ==============================================================================\n",
    "\n",
    "if results:\n",
    "    comparison_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"๐ MODEL COMPARISON TABLE\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Highlight winner\n",
    "    best_idx = comparison_df['Accuracy'].idxmax()\n",
    "    best_model = comparison_df.loc[best_idx, 'Model']\n",
    "    best_acc = comparison_df.loc[best_idx, 'Accuracy']\n",
    "    \n",
    "    print(f\"\\n๐ Best model by accuracy: {best_model} ({best_acc:.1%})\")\n",
    "    print(f\"   Random baseline: 33.3%\")\n",
    "    print(f\"   Improvement: {(best_acc - 0.333)*100:+.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbfbb4f",
   "metadata": {},
   "source": [
    "## 3. Confusion Matrices Side by Side\n",
    "\n",
    "**WHY COMPARE CONFUSION MATRICES?**\n",
    "\n",
    "The confusion matrix shows WHERE each model makes mistakes.\n",
    "This helps us understand if models have different strengths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03f1ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CONFUSION MATRICES FOR ALL MODELS\n",
    "# ==============================================================================\n",
    "\n",
    "# Determine how many models we have\n",
    "n_models = sum([xgb_available, cnn_available, tcn_available])\n",
    "if n_models >= 2:\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 5))\n",
    "    if n_models == 2:\n",
    "        axes = [axes[0], axes[1]]\n",
    "    labels = ['DOWN', 'SIDEWAYS', 'UP']\n",
    "    \n",
    "    ax_idx = 0\n",
    "    \n",
    "    # XGBoost\n",
    "    if xgb_available:\n",
    "        cm_xgb = np.array(xgb_metrics['confusion_matrix'])\n",
    "        sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=labels, yticklabels=labels, ax=axes[ax_idx])\n",
    "        axes[ax_idx].set_xlabel('Predicted')\n",
    "        axes[ax_idx].set_ylabel('Actual')\n",
    "        axes[ax_idx].set_title(f\"XGBoost ({xgb_metrics['accuracy']:.1%})\")\n",
    "        ax_idx += 1\n",
    "    \n",
    "    # CNN-LSTM\n",
    "    if cnn_available:\n",
    "        cm_cnn = np.array(cnn_metrics['confusion_matrix'])\n",
    "        sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Greens',\n",
    "                    xticklabels=labels, yticklabels=labels, ax=axes[ax_idx])\n",
    "        axes[ax_idx].set_xlabel('Predicted')\n",
    "        axes[ax_idx].set_ylabel('Actual')\n",
    "        axes[ax_idx].set_title(f\"CNN-LSTM ({cnn_metrics['accuracy']:.1%})\")\n",
    "        ax_idx += 1\n",
    "    \n",
    "    # TCN-Attention\n",
    "    if tcn_available:\n",
    "        cm_tcn = np.array(tcn_metrics['confusion_matrix'])\n",
    "        sns.heatmap(cm_tcn, annot=True, fmt='d', cmap='Oranges',\n",
    "                    xticklabels=labels, yticklabels=labels, ax=axes[ax_idx])\n",
    "        axes[ax_idx].set_xlabel('Predicted')\n",
    "        axes[ax_idx].set_ylabel('Actual')\n",
    "        axes[ax_idx].set_title(f\"TCN-Attention ({tcn_metrics['accuracy']:.1%})\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comparison_confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037c00c7",
   "metadata": {},
   "source": [
    "## 4. Per-Class Performance\n",
    "\n",
    "**WHY ANALYZE PER-CLASS?**\n",
    "\n",
    "Overall accuracy can hide poor performance on specific classes.\n",
    "For trading, you may care more about UP/DOWN than SIDEWAYS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e04c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PER-CLASS PERFORMANCE BREAKDOWN\n",
    "# ==============================================================================\n",
    "\n",
    "if xgb_available or cnn_available or tcn_available:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"๐ PER-CLASS PERFORMANCE (F1 Scores)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    class_metrics = []\n",
    "    \n",
    "    for cls_name in ['DOWN', 'SIDEWAYS', 'UP']:\n",
    "        row = {'Class': cls_name}\n",
    "        \n",
    "        if xgb_available:\n",
    "            xgb_report = xgb_metrics['classification_report']\n",
    "            xgb_cls = xgb_report.get(cls_name, xgb_report.get(cls_name.lower(), {}))\n",
    "            row['XGB F1'] = xgb_cls.get('f1-score', 0)\n",
    "        \n",
    "        if cnn_available:\n",
    "            cnn_report = cnn_metrics['classification_report']\n",
    "            cnn_cls = cnn_report.get(cls_name, cnn_report.get(cls_name.lower(), {}))\n",
    "            row['CNN F1'] = cnn_cls.get('f1-score', 0)\n",
    "        \n",
    "        if tcn_available:\n",
    "            tcn_report = tcn_metrics['classification_report']\n",
    "            tcn_cls = tcn_report.get(cls_name, tcn_report.get(cls_name.lower(), {}))\n",
    "            row['TCN F1'] = tcn_cls.get('f1-score', 0)\n",
    "        \n",
    "        class_metrics.append(row)\n",
    "    \n",
    "    class_df = pd.DataFrame(class_metrics)\n",
    "    print(\"\\n\" + class_df.to_string(index=False))\n",
    "    \n",
    "    # Find which model wins for each class\n",
    "    print(\"\\n๐ Per-class winners:\")\n",
    "    for idx, row in class_df.iterrows():\n",
    "        xgb_f1 = row['XGB F1']\n",
    "        cnn_f1 = row['CNN F1']\n",
    "        winner = \"XGBoost\" if xgb_f1 > cnn_f1 else \"CNN-LSTM\" if cnn_f1 > xgb_f1 else \"Tie\"\n",
    "        diff = abs(xgb_f1 - cnn_f1)\n",
    "        print(f\"   {row['Class']:8s}: {winner} ({diff:.3f} difference)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bb4e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# BAR CHART COMPARISON\n",
    "# ==============================================================================\n",
    "\n",
    "if xgb_available and cnn_available:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    x = np.arange(3)\n",
    "    width = 0.35\n",
    "    \n",
    "    xgb_f1 = class_df['XGB F1'].values\n",
    "    cnn_f1 = class_df['CNN F1'].values\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, xgb_f1, width, label='XGBoost', color='steelblue')\n",
    "    bars2 = ax.bar(x + width/2, cnn_f1, width, label='CNN-LSTM', color='forestgreen')\n",
    "    \n",
    "    ax.set_xlabel('Class', fontsize=12)\n",
    "    ax.set_ylabel('F1 Score', fontsize=12)\n",
    "    ax.set_title('F1 Score by Class - Model Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(['DOWN', 'SIDEWAYS', 'UP'])\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axhline(y=0.333, color='red', linestyle='--', alpha=0.5, label='Random')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in list(bars1) + list(bars2):\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comparison_f1_by_class.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b4973b",
   "metadata": {},
   "source": [
    "## 5. Statistical Significance Testing\n",
    "\n",
    "**WHY TEST STATISTICAL SIGNIFICANCE?**\n",
    "\n",
    "Just because one model has higher accuracy doesn't mean it's truly better.\n",
    "We use McNemar's test to determine if the difference is statistically significant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f680469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STATISTICAL SIGNIFICANCE TEST (McNemar's Test)\n",
    "# ==============================================================================\n",
    "#\n",
    "# McNemar's test compares paired predictions:\n",
    "# - How often does Model A get right what Model B gets wrong?\n",
    "# - How often does Model B get right what Model A gets wrong?\n",
    "# - If these counts differ significantly, one model is truly better.\n",
    "\n",
    "if xgb_available and cnn_available:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"๐ STATISTICAL SIGNIFICANCE TEST\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # CNN-LSTM uses lookback window so it outputs fewer predictions.\n",
    "    # We need to align the arrays before comparing.\n",
    "    lookback = cnn_model.lookback\n",
    "    \n",
    "    # Trim XGBoost predictions and y_test to match CNN-LSTM length\n",
    "    xgb_preds_aligned = xgb_preds[lookback:]\n",
    "    y_test_aligned = y_test[lookback:]\n",
    "    \n",
    "    # Now both arrays have the same length as cnn_preds\n",
    "    xgb_correct = (xgb_preds_aligned == y_test_aligned)\n",
    "    cnn_correct = (cnn_preds == y_test_aligned)\n",
    "    \n",
    "    # Count the four cases for contingency table\n",
    "    both_correct = np.sum(xgb_correct & cnn_correct)\n",
    "    both_wrong = np.sum(~xgb_correct & ~cnn_correct)\n",
    "    xgb_only_correct = np.sum(xgb_correct & ~cnn_correct)\n",
    "    cnn_only_correct = np.sum(~xgb_correct & cnn_correct)\n",
    "    \n",
    "    print(f\"\"\"\n",
    "Contingency Table:\n",
    "                     CNN-LSTM Correct    CNN-LSTM Wrong\n",
    "    XGBoost Correct     {both_correct:,}              {xgb_only_correct:,}\n",
    "    XGBoost Wrong       {cnn_only_correct:,}              {both_wrong:,}\n",
    "    \n",
    "Key insight:\n",
    "  - XGBoost correct when CNN-LSTM wrong: {xgb_only_correct:,} samples\n",
    "  - CNN-LSTM correct when XGBoost wrong: {cnn_only_correct:,} samples\n",
    "\"\"\")\n",
    "    \n",
    "    # McNemar's test (with continuity correction)\n",
    "    # H0: Both models have the same error rate\n",
    "    # We compare b (XGB only correct) vs c (CNN only correct)\n",
    "    b, c = xgb_only_correct, cnn_only_correct\n",
    "    \n",
    "    if b + c > 0:\n",
    "        # Chi-squared statistic with continuity correction\n",
    "        chi2_stat = (abs(b - c) - 1)**2 / (b + c)\n",
    "        p_value = 1 - stats.chi2.cdf(chi2_stat, df=1)\n",
    "        \n",
    "        print(f\"McNemar's Test Results:\")\n",
    "        print(f\"  Chi-squared statistic: {chi2_stat:.4f}\")\n",
    "        print(f\"  p-value: {p_value:.6f}\")\n",
    "        print(f\"  Significance level: ฮฑ = 0.05\")\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            winner = \"XGBoost\" if b > c else \"CNN-LSTM\"\n",
    "            print(f\"\\n๐ฏ RESULT: Statistically SIGNIFICANT difference (p < 0.05)\")\n",
    "            print(f\"   โ {winner} is significantly better\")\n",
    "        else:\n",
    "            print(f\"\\n๐ฏ RESULT: No statistically significant difference (p >= 0.05)\")\n",
    "            print(f\"   โ Models perform similarly, difference may be due to chance\")\n",
    "    else:\n",
    "        print(\"โ๏ธ Cannot perform McNemar's test: no discordant pairs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c357e",
   "metadata": {},
   "source": [
    "## 6. Predictions Visualization\n",
    "\n",
    "**WHY VISUALIZE PREDICTIONS?**\n",
    "\n",
    "Seeing predictions overlaid on price helps us understand:\n",
    "- Are predictions stable or noisy?\n",
    "- Does the model catch major trends?\n",
    "- Where does it make mistakes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61942bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PREDICTIONS ON PRICE CHART\n",
    "# ==============================================================================\n",
    "\n",
    "if xgb_available:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"๐ PREDICTIONS VISUALIZATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Sample last 500 points for visualization\n",
    "    sample_size = 500\n",
    "    test_sample = test_df.tail(sample_size).copy().reset_index(drop=True)\n",
    "    \n",
    "    # Align predictions with test sample\n",
    "    xgb_sample_preds = xgb_preds[-sample_size:]\n",
    "    actual_sample = y_test[-sample_size:]\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "    \n",
    "    # Color scheme for labels\n",
    "    colors = {0: '#ff6b6b', 1: '#a0a0a0', 2: '#51cf66'}\n",
    "    label_names = {0: 'DOWN', 1: 'SIDEWAYS', 2: 'UP'}\n",
    "    \n",
    "    # Plot 1: Price chart\n",
    "    axes[0].plot(test_sample['time'], test_sample['close'], \n",
    "                 linewidth=0.8, color='steelblue')\n",
    "    axes[0].set_ylabel('Price (USD)', fontsize=11)\n",
    "    axes[0].set_title('BTC Close Price (Test Period)', fontsize=12, fontweight='bold')\n",
    "    axes[0].ticklabel_format(style='plain', axis='y')\n",
    "    \n",
    "    # Plot 2: Actual labels\n",
    "    for label in [0, 1, 2]:\n",
    "        mask = actual_sample == label\n",
    "        axes[1].fill_between(test_sample['time'], 0, 1, where=mask,\n",
    "                            color=colors[label], alpha=0.7, label=label_names[label])\n",
    "    axes[1].set_ylabel('Actual', fontsize=11)\n",
    "    axes[1].set_title('Actual Direction Labels', fontsize=12, fontweight='bold')\n",
    "    axes[1].legend(loc='upper right', ncol=3)\n",
    "    axes[1].set_yticks([])\n",
    "    \n",
    "    # Plot 3: XGBoost predictions\n",
    "    for label in [0, 1, 2]:\n",
    "        mask = xgb_sample_preds == label\n",
    "        axes[2].fill_between(test_sample['time'], 0, 1, where=mask,\n",
    "                            color=colors[label], alpha=0.7, label=label_names[label])\n",
    "    axes[2].set_ylabel('XGBoost', fontsize=11)\n",
    "    axes[2].set_xlabel('Time', fontsize=11)\n",
    "    axes[2].set_title('XGBoost Predictions', fontsize=12, fontweight='bold')\n",
    "    axes[2].legend(loc='upper right', ncol=3)\n",
    "    axes[2].set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comparison_predictions_chart.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n๐ก INTERPRETATION:\")\n",
    "    print(\"   - Compare middle (actual) and bottom (predicted) panels\")\n",
    "    print(\"   - Good alignment = model captures real trends\")\n",
    "    print(\"   - Frequent color changes in predictions = noisy model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58afedd7",
   "metadata": {},
   "source": [
    "## 7. Multi-Horizon Comparison (Optional)\n",
    "\n",
    "If you trained models for different horizons (1, 3, 5 bars),\n",
    "this section compares them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f2b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# HORIZON COMPARISON\n",
    "# ==============================================================================\n",
    "#\n",
    "# Compare performance across different prediction horizons.\n",
    "# Longer horizons are generally harder to predict.\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"๐ฎ MULTI-HORIZON COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(\"Checking for models trained on different horizons...\")\n",
    "print(f\"   TCN module available: {tcn_module_available}\\n\")\n",
    "\n",
    "horizons = [1, 3, 5]\n",
    "horizon_results = []\n",
    "\n",
    "for h in horizons:\n",
    "    # Prepare data for this horizon\n",
    "    df_h, _ = prepare_features(df, horizon=h)\n",
    "    _, _, test_h = split_data_by_time(df_h, train_end=\"2025-06-30\", test_start=\"2025-07-01\")\n",
    "    \n",
    "    f_cols = [c for c in feature_cols if c in test_h.columns]\n",
    "    X_h = test_h[f_cols].values\n",
    "    y_h = test_h['target'].values.astype(int)\n",
    "    \n",
    "    # Try loading XGBoost for this horizon\n",
    "    try:\n",
    "        xgb_h = XGBBaseline.load(MODEL_DIR, name=f'xgb_baseline_h{h}')\n",
    "        metrics_h = xgb_h.evaluate(X_h, y_h)\n",
    "        horizon_results.append({\n",
    "            'Horizon': f'{h} bar(s)',\n",
    "            'Model': 'XGBoost',\n",
    "            'Accuracy': metrics_h['accuracy'],\n",
    "            'F1': metrics_h['f1_weighted']\n",
    "        })\n",
    "        print(f\"โ XGBoost H={h}: Acc={metrics_h['accuracy']:.4f}\")\n",
    "    except Exception:\n",
    "        print(f\"   XGBoost H={h}: Not found\")\n",
    "    \n",
    "    # Try loading CNN-LSTM for this horizon\n",
    "    try:\n",
    "        cnn_h = CNNLSTMModel.load(MODEL_DIR, name=f'cnn_lstm_h{h}', device='cuda')\n",
    "        metrics_h = cnn_h.evaluate(X_h, y_h)\n",
    "        horizon_results.append({\n",
    "            'Horizon': f'{h} bar(s)',\n",
    "            'Model': 'CNN-LSTM',\n",
    "            'Accuracy': metrics_h['accuracy'],\n",
    "            'F1': metrics_h['f1_weighted']\n",
    "        })\n",
    "        print(f\"โ CNN-LSTM H={h}: Acc={metrics_h['accuracy']:.4f}\")\n",
    "    except Exception:\n",
    "        print(f\"   CNN-LSTM H={h}: Not found\")\n",
    "    \n",
    "    # Try loading TCN-Attention for this horizon\n",
    "    if tcn_module_available:\n",
    "        try:\n",
    "            tcn_h = TCNAttentionModel.load(MODEL_DIR, name=f'tcn_attention_h{h}', device='cuda')\n",
    "            metrics_h = tcn_h.evaluate(X_h, y_h)\n",
    "            horizon_results.append({\n",
    "                'Horizon': f'{h} bar(s)',\n",
    "                'Model': 'TCN-Attention',\n",
    "                'Accuracy': metrics_h['accuracy'],\n",
    "                'F1': metrics_h['f1_weighted']\n",
    "            })\n",
    "            print(f\"โ TCN-Attention H={h}: Acc={metrics_h['accuracy']:.4f}\")\n",
    "        except Exception:\n",
    "            print(f\"   TCN-Attention H={h}: Not found\")\n",
    "\n",
    "if horizon_results:\n",
    "    horizon_df = pd.DataFrame(horizon_results)\n",
    "    print(\"\\n\" + horizon_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nโ๏ธ Only horizon=1 models found. Train with different horizons to compare.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb10e49",
   "metadata": {},
   "source": [
    "## 8. Final Conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285dbdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FINAL CONCLUSIONS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"๐ FINAL CONCLUSIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if results:\n",
    "    best_model = comparison_df.loc[comparison_df['Accuracy'].idxmax(), 'Model']\n",
    "    best_acc = comparison_df['Accuracy'].max()\n",
    "    best_f1 = comparison_df.loc[comparison_df['Accuracy'].idxmax(), 'F1 Weighted']\n",
    "    \n",
    "    # Calculate accuracy difference\n",
    "    if len(comparison_df) > 1:\n",
    "        acc_diff = abs(comparison_df['Accuracy'].iloc[0] - comparison_df['Accuracy'].iloc[1])\n",
    "    else:\n",
    "        acc_diff = 0\n",
    "\n",
    "    print(f\"\"\"\n",
    "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ\n",
    "๐ WINNER\n",
    "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ\n",
    "โข Best Model: {best_model}\n",
    "โข Accuracy: {best_acc:.4f} ({best_acc:.1%})\n",
    "โข F1 Score: {best_f1:.4f}\n",
    "\n",
    "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ\n",
    "๐ KEY FINDINGS\n",
    "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ\n",
    "1. Both models beat random baseline (33.3%)\n",
    "   โ ML provides real predictive value\n",
    "\n",
    "2. Accuracy difference: {acc_diff:.1%}\n",
    "   โ {'Models are very close' if acc_diff < 0.02 else 'Clear winner exists'}\n",
    "\n",
    "3. Oracle labels provide clean training targets\n",
    "   โ Gaussian smoothing removes noise effectively\n",
    "\n",
    "4. Technical indicators capture useful patterns\n",
    "   โ Feature engineering is crucial\n",
    "\n",
    "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ\n",
    "๐ก RECOMMENDATIONS\n",
    "โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ\n",
    "1. For Production: Use {best_model}\n",
    "   - Highest accuracy on test set\n",
    "   - Proven performance on unseen data\n",
    "\n",
    "2. For Higher Confidence: Consider ensemble\n",
    "   - Average predictions from both models\n",
    "   - May reduce variance\n",
    "\n",
    "3. Monitor Performance: Watch for drift\n",
    "   - Retrain periodically (monthly/quarterly)\n",
    "   - Market conditions change over time\n",
    "\n",
    "4. Trading Integration:\n",
    "   - Don't trade on SIDEWAYS predictions\n",
    "   - Use confidence scores to filter trades\n",
    "   - Backtest thoroughly before live trading\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2fc862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SAVE COMPARISON RESULTS\n",
    "# ==============================================================================\n",
    "\n",
    "if results:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"๐พ SAVING RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create reports directory\n",
    "    Path('reports').mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save comparison CSV\n",
    "    comparison_df.to_csv('reports/model_comparison.csv', index=False)\n",
    "    print(f\"โ Comparison table: reports/model_comparison.csv\")\n",
    "    \n",
    "    # Save summary JSON\n",
    "    summary = {\n",
    "        'best_model': best_model,\n",
    "        'best_accuracy': float(best_acc),\n",
    "        'best_f1': float(best_f1),\n",
    "        'random_baseline': 0.333,\n",
    "        'improvement': float(best_acc - 0.333),\n",
    "        'models': results,\n",
    "        'horizon': HORIZON\n",
    "    }\n",
    "    \n",
    "    with open('reports/comparison_summary.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"โ Summary JSON: reports/comparison_summary.json\")\n",
    "    \n",
    "    # Save per-class metrics if available\n",
    "    if xgb_available and cnn_available:\n",
    "        class_df.to_csv('reports/per_class_metrics.csv', index=False)\n",
    "        print(f\"โ Per-class metrics: reports/per_class_metrics.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"โ COMPARISON COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAll notebooks completed. See reports/ directory for saved results.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
