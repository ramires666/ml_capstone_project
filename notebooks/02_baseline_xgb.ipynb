{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dcf3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==============================================================================\n",
    "02 - BASELINE XGBOOST MODEL\n",
    "==============================================================================\n",
    "\n",
    "PURPOSE OF THIS NOTEBOOK:\n",
    "-------------------------\n",
    "Train and evaluate the XGBoost baseline model for price direction prediction.\n",
    "\n",
    "XGBoost is our BASELINE because:\n",
    "1. It's a proven algorithm for tabular data (features in columns)\n",
    "2. Fast training with GPU support\n",
    "3. Handles missing values automatically\n",
    "4. Provides feature importance (interpretability)\n",
    "5. Works well out-of-the-box with minimal tuning\n",
    "\n",
    "\n",
    "------------------\n",
    "- How to prepare data for ML training\n",
    "- Walk-forward cross-validation (crucial for time series!)\n",
    "- Hyperparameter tuning with RandomizedSearchCV\n",
    "- How to interpret feature importance\n",
    "- Proper evaluation with confusion matrix\n",
    "\n",
    "EXPECTED RESULTS:\n",
    "-----------------\n",
    "For 3-class classification (UP/SIDEWAYS/DOWN):\n",
    "- Random baseline: ~33% accuracy\n",
    "- Our goal: >40% accuracy and F1 > 0.35\n",
    "- Any improvement over random is valuable for trading\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dde871",
   "metadata": {},
   "source": [
    "# XGBoost Baseline Model\n",
    "\n",
    "**Goal**: Train a baseline classifier that beats random guessing (33%).\n",
    "\n",
    "**Why XGBoost?**\n",
    "- Best algorithm for tabular data\n",
    "- GPU acceleration for fast training\n",
    "- Interpretable feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948dacf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# ==============================================================================\n",
    "# \n",
    "\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path for importing our modules\n",
    "# Use Path(__file__) to get correct path regardless of CWD\n",
    "project_root = Path(__file__).resolve().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6db8842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# IMPORT PROJECT MODULES\n",
    "# ==============================================================================\n",
    "#\n",
    "# Our custom modules:\n",
    "# - load_and_merge_data: Combines klines, funding, volume data\n",
    "# - split_data_by_time: Time-based train/val/test split (NOT random!)\n",
    "# - create_oracle_labels: Generates UP/DOWN/SIDEWAYS targets\n",
    "# - prepare_features: Adds 60+ technical indicators\n",
    "# - get_indicator_columns: Filters feature column names\n",
    "# - XGBBaseline: Our XGBoost wrapper with GPU support\n",
    "\n",
    "# Reload modules to pick up any code changes without restarting kernel\n",
    "import importlib\n",
    "import src.data.loader\n",
    "import src.labeling.oracle\n",
    "import src.features.builder\n",
    "import src.features.indicators\n",
    "import src.models.xgb\n",
    "importlib.reload(src.data.loader)\n",
    "importlib.reload(src.labeling.oracle)\n",
    "importlib.reload(src.features.builder)\n",
    "importlib.reload(src.features.indicators)\n",
    "importlib.reload(src.models.xgb)\n",
    "\n",
    "from src.data.loader import load_and_merge_data, split_data_by_time\n",
    "from src.labeling.oracle import create_oracle_labels\n",
    "from src.features.builder import prepare_features\n",
    "from src.features.indicators import get_indicator_columns\n",
    "from src.models.xgb import XGBBaseline, print_classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6804019a",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "**IMPORTANT**: All key parameters are defined here for easy experimentation.\n",
    "\n",
    "**Parameter Explanations:**\n",
    "- `SIGMA`: Oracle smoothing strength (4 is good for 15-min candles)\n",
    "- `THRESHOLD`: Minimum slope for UP/DOWN labels (lower = more UP/DOWN, fewer SIDEWAYS)\n",
    "- `HORIZON`: How many bars ahead to predict (1 = next bar)\n",
    "- `TRAIN_END`: Last date for training data\n",
    "- `TEST_START`: First date for test data (gap prevents look-ahead bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b817882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CONFIGURATION PARAMETERS\n",
    "# ==============================================================================\n",
    "# \n",
    "# These are the MAIN settings to experiment with.\n",
    "# Change these and re-run to see different results.\n",
    "\n",
    "# Oracle label parameters\n",
    "SIGMA = 3           # Gaussian smoothing sigma (higher = smoother trends)\n",
    "THRESHOLD = 0.0002  # Slope threshold for direction classification\n",
    "\n",
    "# Prediction horizon\n",
    "HORIZON = 1         # Predict direction 1 bar (15 min) ahead\n",
    "                    # Change to 3 or 5 for longer-term predictions\n",
    "\n",
    "# Train/Test split dates\n",
    "TRAIN_END = \"2025-06-30\"    # Train on data up to this date\n",
    "TEST_START = \"2025-07-01\"   # Test on data from this date onwards\n",
    "                            # Gap ensures no data leakage\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“‹ CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Oracle Parameters: sigma={SIGMA}, threshold={THRESHOLD}\")\n",
    "print(f\"Prediction Horizon: {HORIZON} bar(s) = {HORIZON * 15} minutes\")\n",
    "print(f\"Training Period: up to {TRAIN_END}\")\n",
    "print(f\"Testing Period: from {TEST_START}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c462fb8",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data\n",
    "\n",
    "**Data Pipeline:**\n",
    "1. Load raw OHLCV + funding + volume data\n",
    "2. Create oracle labels (target variable)\n",
    "3. Add 60+ technical indicators (features)\n",
    "4. Split into train/validation/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d732424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 1: LOAD RAW DATA\n",
    "# ==============================================================================\n",
    "#\n",
    "# WHY: We need the base price data to create features and labels.\n",
    "#\n",
    "# The function combines:\n",
    "# - Klines: OHLCV candlestick data\n",
    "# - Funding: Perpetual futures funding rates\n",
    "# - Volumes: Buy/sell volume breakdown\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“¥ STEP 1: LOADING DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df = load_and_merge_data(end_date='2025-12-31')\n",
    "print(f\"\\nâœ… Loaded {len(df):,} rows of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941b7264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 2: CREATE ORACLE LABELS\n",
    "# ==============================================================================\n",
    "#\n",
    "# WHY: The oracle creates clean training targets using Gaussian smoothing.\n",
    "#\n",
    "# How it works:\n",
    "# 1. Smooth the price series with Gaussian filter\n",
    "# 2. Calculate slope of smoothed line\n",
    "# 3. Classify as UP (positive slope > threshold), \n",
    "#    DOWN (negative slope < -threshold), or SIDEWAYS\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¯ STEP 2: CREATING TARGET LABELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df = create_oracle_labels(df, sigma=SIGMA, threshold=THRESHOLD)\n",
    "\n",
    "print(\"\\nLabel Distribution:\")\n",
    "label_counts = df['target'].value_counts(normalize=True).sort_index()\n",
    "label_names = {0: 'DOWN', 1: 'SIDEWAYS', 2: 'UP'}\n",
    "for label, pct in label_counts.items():\n",
    "    bar = \"â–ˆ\" * int(pct * 50)\n",
    "    print(f\"  {label_names[label]:8s}: {pct*100:5.1f}% {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0ed41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 3: GENERATE FEATURES\n",
    "# ==============================================================================\n",
    "#\n",
    "# WHY: Technical indicators capture patterns that may predict direction.\n",
    "#\n",
    "# prepare_features() adds 60+ indicators:\n",
    "# - Momentum: RSI, MACD, Stochastic, Williams %R, etc.\n",
    "# - Trend: EMA, SMA, ADX, Aroon, etc.\n",
    "# - Volatility: ATR, Bollinger Bands, etc.\n",
    "# - Volume: OBV, CMF, MFI, etc.\n",
    "#\n",
    "# The 'horizon' parameter shifts the target by N bars to prevent data leakage.\n",
    "# When horizon=1, we predict the NEXT bar's direction using current features.\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âš™ï¸ STEP 3: GENERATING FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_features, group_map = prepare_features(df, horizon=HORIZON)\n",
    "\n",
    "print(\"\\nFeatures generated by category:\")\n",
    "total_features = 0\n",
    "for group, cols in group_map.items():\n",
    "    print(f\"  {group:12s}: {len(cols):3d} features\")\n",
    "    total_features += len(cols)\n",
    "print(f\"  {'TOTAL':12s}: {total_features:3d} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26e03ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 4: TIME-BASED TRAIN/VALIDATION/TEST SPLIT\n",
    "# ==============================================================================\n",
    "#\n",
    "# CRITICAL: For time series, NEVER use random splitting!\n",
    "#\n",
    "# Why time-based split?\n",
    "# - Random split causes data leakage (future data in training)\n",
    "# - Time-based split simulates real trading scenario\n",
    "# - Model sees past, predicts future (just like in production)\n",
    "#\n",
    "# Split structure:\n",
    "# |---- Training ----|-- Val --|---- Testing ----|\n",
    "# |  Jan 2024 -----> Jun 2025  |  Jul 2025 ----> |\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š STEP 4: SPLITTING DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_df, val_df, test_df = split_data_by_time(\n",
    "    df_features,\n",
    "    train_end=TRAIN_END,\n",
    "    test_start=TEST_START,\n",
    "    val_ratio=0.1  # Last 10% of training data for validation\n",
    ")\n",
    "\n",
    "# Get list of feature columns (exclude target and metadata)\n",
    "feature_cols = get_indicator_columns(\n",
    "    df_features, \n",
    "    exclude_cols=['time', 'target', 'smoothed_close', 'smooth_slope']\n",
    ")\n",
    "feature_cols = [c for c in feature_cols if c in train_df.columns]\n",
    "\n",
    "print(f\"\\nâœ… Features selected: {len(feature_cols)}\")\n",
    "print(\"\\nğŸ“‹ FULL FEATURE LIST:\")\n",
    "for i, col in enumerate(sorted(feature_cols), 1):\n",
    "    print(f\"   {i:3d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d2967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 5: EXTRACT FEATURE MATRICES\n",
    "# ==============================================================================\n",
    "#\n",
    "# WHY: Convert DataFrames to numpy arrays for XGBoost.\n",
    "#\n",
    "# X = feature matrix (samples Ã— features)\n",
    "# y = target vector (class labels: 0, 1, 2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“ STEP 5: EXTRACTING FEATURE MATRICES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Training data\n",
    "X_train = train_df[feature_cols].values\n",
    "y_train = train_df['target'].values.astype(int)\n",
    "\n",
    "# Validation data (for early stopping and hyperparameter tuning)\n",
    "X_val = val_df[feature_cols].values\n",
    "y_val = val_df['target'].values.astype(int)\n",
    "\n",
    "# Test data (final evaluation - NEVER peek during training!)\n",
    "X_test = test_df[feature_cols].values\n",
    "y_test = test_df['target'].values.astype(int)\n",
    "\n",
    "# Clean inf/nan values (some indicators like EOM produce inf from division by zero)\n",
    "X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "X_val = np.nan_to_num(X_val, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "X_test = np.nan_to_num(X_test, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "print(f\"Training set:   {X_train.shape[0]:,} samples Ã— {X_train.shape[1]} features\")\n",
    "print(f\"Validation set: {X_val.shape[0]:,} samples Ã— {X_val.shape[1]} features\")\n",
    "print(f\"Test set:       {X_test.shape[0]:,} samples Ã— {X_test.shape[1]} features\")\n",
    "\n",
    "# Verify no NaN values (XGBoost handles them but let's be safe)\n",
    "nan_count = np.isnan(X_train).sum()\n",
    "if nan_count > 0:\n",
    "    print(f\"\\nâš ï¸ Warning: {nan_count} NaN values in training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67adf02",
   "metadata": {},
   "source": [
    "## 3. Train Baseline Model (No Tuning)\n",
    "\n",
    "**WHY TRAIN WITHOUT TUNING FIRST?**\n",
    "- Get a baseline performance quickly\n",
    "- Verify the pipeline works correctly\n",
    "- Compare improvement after tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e920590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TRAIN MODEL WITH DEFAULT PARAMETERS\n",
    "# ==============================================================================\n",
    "#\n",
    "# XGBBaseline is our wrapper around XGBClassifier:\n",
    "# - Handles multi-class classification (3 classes)\n",
    "# - Uses GPU if available (device='cuda')\n",
    "# - Provides consistent interface for training/evaluation\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ TRAINING BASELINE MODEL (DEFAULT PARAMETERS)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model = XGBBaseline(\n",
    "    n_classes=3,        # DOWN=0, SIDEWAYS=1, UP=2\n",
    "    device='cuda',      # Use GPU for faster training\n",
    "    random_state=42     # For reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "# The fit() method:\n",
    "# 1. Creates XGBClassifier with default params\n",
    "# 2. Fits on training data\n",
    "# 3. Uses validation data for early stopping\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    feature_names=feature_cols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0de387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EVALUATE ON VALIDATION SET\n",
    "# ==============================================================================\n",
    "#\n",
    "# WHY EVALUATE ON VALIDATION (not test)?\n",
    "# - Test set should only be used ONCE at the end\n",
    "# - Validation set is for comparing models and tuning\n",
    "# - This prevents overfitting to the test set\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š VALIDATION RESULTS (BEFORE TUNING)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "val_metrics = model.evaluate(X_val, y_val)\n",
    "print_classification_report(val_metrics, \"Validation Results (No Tuning)\")\n",
    "\n",
    "print(\"\\nğŸ’¡ INTERPRETATION:\")\n",
    "print(f\"  - Accuracy: {val_metrics['accuracy']:.1%} (random baseline: 33.3%)\")\n",
    "print(f\"  - If accuracy > 33.3%, model learned something!\")\n",
    "print(f\"  - F1-weighted: {val_metrics['f1_weighted']:.3f} (considers class imbalance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0af4c8",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning\n",
    "\n",
    "**WHY TUNE HYPERPARAMETERS?**\n",
    "\n",
    "XGBoost has many parameters that affect performance:\n",
    "- `n_estimators`: Number of trees (more = more capacity)\n",
    "- `max_depth`: Tree depth (deeper = more complex patterns)\n",
    "- `learning_rate`: Step size (smaller = more careful learning)\n",
    "- `subsample`: Row sampling (prevents overfitting)\n",
    "\n",
    "**RandomizedSearchCV**: Tries random combinations and picks the best.\n",
    "Using TimeSeriesSplit for cross-validation (respects time order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a77ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# HYPERPARAMETER TUNING WITH RANDOMIZED SEARCH\n",
    "# ==============================================================================\n",
    "#\n",
    "# This step takes a few minutes (trying 25 combinations with 5-fold CV).\n",
    "#\n",
    "# TimeSeriesSplit creates folds like:\n",
    "# Fold 1: Train on [0-100], Test on [100-120]\n",
    "# Fold 2: Train on [0-120], Test on [120-140]\n",
    "# ...\n",
    "# This respects time order and simulates walk-forward testing.\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ”§ HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*60)\n",
    "print(\"This may take a few minutes...\")\n",
    "print(\"Trying 25 random combinations with 5-fold TimeSeriesSplit CV\\n\")\n",
    "\n",
    "best_params = model.tune(\n",
    "    X_train, y_train,\n",
    "    n_iter=25,          # Try 25 random combinations\n",
    "    cv_splits=5,        # 5-fold time series cross-validation\n",
    "    scoring='f1_weighted'  # Optimize for weighted F1 score\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Best parameters found:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cacb88",
   "metadata": {},
   "source": [
    "## 5. Final Evaluation on Test Set\n",
    "\n",
    "**THIS IS THE MOMENT OF TRUTH!**\n",
    "\n",
    "We evaluate on the test set ONLY ONCE after all tuning is complete.\n",
    "This gives us an unbiased estimate of real-world performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcbdfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EVALUATE ON TEST SET\n",
    "# ==============================================================================\n",
    "#\n",
    "# The test set was held out during training and tuning.\n",
    "# This evaluation simulates how the model would perform in production.\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¯ FINAL TEST SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_metrics = model.evaluate(X_test, y_test)\n",
    "print_classification_report(test_metrics, \"Test Results (After Tuning)\")\n",
    "\n",
    "# Compare to baseline\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"ğŸ“ˆ PERFORMANCE SUMMARY\")\n",
    "print(\"-\"*40)\n",
    "print(f\"Test Accuracy:    {test_metrics['accuracy']:.1%}\")\n",
    "print(f\"Random Baseline:  33.3%\")\n",
    "print(f\"Improvement:      {(test_metrics['accuracy'] - 0.333)*100:+.1f}%\")\n",
    "print(f\"F1 Weighted:      {test_metrics['f1_weighted']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33b1130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CONFUSION MATRIX VISUALIZATION\n",
    "# ==============================================================================\n",
    "#\n",
    "# WHY: Shows where the model makes mistakes.\n",
    "#\n",
    "# Reading the matrix:\n",
    "# - Rows = Actual classes\n",
    "# - Columns = Predicted classes\n",
    "# - Diagonal = Correct predictions\n",
    "# - Off-diagonal = Mistakes\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "cm = np.array(test_metrics['confusion_matrix'])\n",
    "labels = ['DOWN', 'SIDEWAYS', 'UP']\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=labels, yticklabels=labels, ax=ax,\n",
    "            annot_kws={\"size\": 14})\n",
    "\n",
    "ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax.set_ylabel('Actual Label', fontsize=12)\n",
    "ax.set_title('Confusion Matrix - XGBoost Baseline', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('baseline_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Interpret the matrix\n",
    "print(\"\\nğŸ’¡ CONFUSION MATRIX INTERPRETATION:\")\n",
    "print(f\"  - Diagonal values (correct): {cm.diagonal().sum()} predictions\")\n",
    "print(f\"  - Off-diagonal (errors): {cm.sum() - cm.diagonal().sum()} predictions\")\n",
    "\n",
    "# Find most common error\n",
    "off_diag = cm.copy()\n",
    "np.fill_diagonal(off_diag, 0)\n",
    "max_error_idx = np.unravel_index(off_diag.argmax(), off_diag.shape)\n",
    "print(f\"  - Most common error: Actual={labels[max_error_idx[0]]} â†’ Predicted={labels[max_error_idx[1]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0d40e5",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis\n",
    "\n",
    "**WHY ANALYZE FEATURE IMPORTANCE?**\n",
    "\n",
    "- Understand what drives predictions\n",
    "- Identify potentially useless features\n",
    "- Guide future feature engineering\n",
    "- Explain model to stakeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190e91e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FEATURE IMPORTANCE VISUALIZATION\n",
    "# ==============================================================================\n",
    "#\n",
    "# XGBoost calculates feature importance based on:\n",
    "# - How often a feature is used in splits (gain)\n",
    "# - How much it improves predictions (weight)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ” FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get feature importance DataFrame\n",
    "fi = model.get_feature_importance()\n",
    "\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "print(fi.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df00e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top 20 features\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "top_20 = fi.head(20)\n",
    "colors = plt.cm.Blues(np.linspace(0.4, 0.9, len(top_20)))[::-1]\n",
    "\n",
    "bars = ax.barh(range(len(top_20)), top_20['Importance'].values, color=colors)\n",
    "ax.set_yticks(range(len(top_20)))\n",
    "ax.set_yticklabels(top_20['Feature'].values, fontsize=10)\n",
    "ax.invert_yaxis()  # Most important at top\n",
    "ax.set_xlabel('Importance (Gain)', fontsize=12)\n",
    "ax.set_title('Top 20 Features by Importance', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add value labels\n",
    "for bar, imp in zip(bars, top_20['Importance'].values):\n",
    "    ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "            f'{imp:.3f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('baseline_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ INTERPRETATION:\")\n",
    "print(\"  - Higher importance = feature is more useful for predictions\")\n",
    "print(\"  - Momentum/Trend indicators often dominate\")\n",
    "print(\"  - Low-importance features could be removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dba4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SAVE MODEL AND ARTIFACTS\n",
    "# ==============================================================================\n",
    "#\n",
    "# WHY SAVE?\n",
    "# - Reuse model for inference without retraining\n",
    "# - Compare different model versions\n",
    "# - Deploy to production\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ’¾ SAVING MODEL AND ARTIFACTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create output directory\n",
    "Path('models_artifacts').mkdir(exist_ok=True)\n",
    "Path('reports').mkdir(exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model.save('models_artifacts', name=f'xgb_baseline_h{HORIZON}')\n",
    "print(f\"âœ… Model saved: models_artifacts/xgb_baseline_h{HORIZON}_model.joblib\")\n",
    "\n",
    "# Save feature importance\n",
    "fi.to_csv(f'models_artifacts/feature_importance_h{HORIZON}.csv', index=False)\n",
    "print(f\"âœ… Feature importance saved: models_artifacts/feature_importance_h{HORIZON}.csv\")\n",
    "\n",
    "# Save metrics\n",
    "import json\n",
    "with open(f'reports/baseline_metrics_h{HORIZON}.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'accuracy': test_metrics['accuracy'],\n",
    "        'f1_weighted': test_metrics['f1_weighted'],\n",
    "        'f1_macro': test_metrics['f1_macro'],\n",
    "        'best_params': best_params\n",
    "    }, f, indent=2)\n",
    "print(f\"âœ… Metrics saved: reports/baseline_metrics_h{HORIZON}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e499faba",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d217230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“‹ BASELINE MODEL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "âš™ï¸ CONFIGURATION\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "â€¢ Oracle: sigma={SIGMA}, threshold={THRESHOLD}\n",
    "â€¢ Horizon: {HORIZON} bar(s) ({HORIZON * 15} minutes)\n",
    "â€¢ Training: up to {TRAIN_END}\n",
    "â€¢ Testing: from {TEST_START}\n",
    "\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "ğŸ“Š RESULTS\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "â€¢ Test Accuracy:  {test_metrics['accuracy']:.4f} ({test_metrics['accuracy']:.1%})\n",
    "â€¢ F1 Weighted:    {test_metrics['f1_weighted']:.4f}\n",
    "â€¢ F1 Macro:       {test_metrics['f1_macro']:.4f}\n",
    "â€¢ Random Baseline: 33.3%\n",
    "\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "ğŸ”§ BEST HYPERPARAMETERS\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "{chr(10).join(f'â€¢ {k}: {v}' for k, v in best_params.items())}\n",
    "\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "ğŸ’¾ SAVED FILES\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "â€¢ Model: models_artifacts/xgb_baseline_h{HORIZON}_model.joblib\n",
    "â€¢ Feature Importance: models_artifacts/feature_importance_h{HORIZON}.csv\n",
    "â€¢ Metrics: reports/baseline_metrics_h{HORIZON}.json\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… BASELINE TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext step: Run 03_cnn_lstm.py to train the advanced model.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
