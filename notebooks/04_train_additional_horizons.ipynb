{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a737274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n==============================================================================\\n04 - TRAIN MODELS FOR ADDITIONAL HORIZONS (3 and 5 bars ahead)\\n==============================================================================\\n\\nPURPOSE OF THIS SCRIPT:\\n-----------------------\\nIn the 04_comparison notebook, we discovered that models for horizons 3 and 5\\nwere missing. This script trains those models so we can compare performance\\nacross different prediction horizons.\\n\\nWHY DIFFERENT HORIZONS MATTER:\\n------------------------------\\n- Horizon=1: Predict price direction 15 minutes ahead (1 bar)\\n- Horizon=3: Predict price direction 45 minutes ahead (3 bars)  \\n- Horizon=5: Predict price direction 75 minutes ahead (5 bars)\\n\\nGenerally, longer horizons are HARDER to predict because more things can \\nhappen in a longer time window. But sometimes longer trends are more stable\\nthan short-term noise.\\n\\nHOW TO RUN THIS SCRIPT:\\n-----------------------\\nOpen a terminal in WSL with conda environment activated:\\n\\n    cd /mnt/c/_PYTH/projects/capstone_project/notebooks\\n    conda activate btc\\n    python train_horizons_3_5.py\\n\\nThis will train 4 models (2 horizons Ã— 2 model types) and save them to:\\n    models_artifacts/xgb_baseline_h3_model.joblib\\n    models_artifacts/xgb_baseline_h5_model.joblib\\n    models_artifacts/cnn_lstm_h3_model.keras\\n    models_artifacts/cnn_lstm_h5_model.keras\\n\\nESTIMATED TIME:\\n---------------\\n- XGBoost: ~5-10 minutes per horizon (includes hyperparameter tuning)\\n- CNN-LSTM: ~10-20 minutes per horizon (depends on GPU speed)\\n- Total: approximately 30-60 minutes for all 4 models\\n\\nAFTER RUNNING THIS SCRIPT:\\n--------------------------\\nRe-run the 04_comparison notebook to see the full horizon comparison table.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "==============================================================================\n",
    "04 - TRAIN MODELS FOR ADDITIONAL HORIZONS (3 and 5 bars ahead)\n",
    "==============================================================================\n",
    "\n",
    "PURPOSE OF THIS SCRIPT:\n",
    "-----------------------\n",
    "In the 04_comparison notebook, we discovered that models for horizons 3 and 5\n",
    "were missing. This script trains those models so we can compare performance\n",
    "across different prediction horizons.\n",
    "\n",
    "WHY DIFFERENT HORIZONS MATTER:\n",
    "------------------------------\n",
    "- Horizon=1: Predict price direction 15 minutes ahead (1 bar)\n",
    "- Horizon=3: Predict price direction 45 minutes ahead (3 bars)  \n",
    "- Horizon=5: Predict price direction 75 minutes ahead (5 bars)\n",
    "\n",
    "Generally, longer horizons are HARDER to predict because more things can \n",
    "happen in a longer time window. But sometimes longer trends are more stable\n",
    "than short-term noise.\n",
    "\n",
    "HOW TO RUN THIS SCRIPT:\n",
    "-----------------------\n",
    "Open a terminal in WSL with conda environment activated:\n",
    "\n",
    "    cd /mnt/c/_PYTH/projects/capstone_project/notebooks\n",
    "    conda activate btc\n",
    "    python train_horizons_3_5.py\n",
    "\n",
    "This will train 4 models (2 horizons Ã— 2 model types) and save them to:\n",
    "    models_artifacts/xgb_baseline_h3_model.joblib\n",
    "    models_artifacts/xgb_baseline_h5_model.joblib\n",
    "    models_artifacts/cnn_lstm_h3_model.keras\n",
    "    models_artifacts/cnn_lstm_h5_model.keras\n",
    "\n",
    "ESTIMATED TIME:\n",
    "---------------\n",
    "- XGBoost: ~5-10 minutes per horizon (includes hyperparameter tuning)\n",
    "- CNN-LSTM: ~10-20 minutes per horizon (depends on GPU speed)\n",
    "- Total: approximately 30-60 minutes for all 4 models\n",
    "\n",
    "AFTER RUNNING THIS SCRIPT:\n",
    "--------------------------\n",
    "Re-run the 04_comparison notebook to see the full horizon comparison table.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c78284",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "STEP 0: IMPORTS AND SETUP\n",
    "==============================================================================\n",
    "\n",
    "We need to import all the same modules we use in the training notebooks.\n",
    "This ensures consistency between notebook training and script training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696d46b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c02bc289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the project root directory to Python's path.\n",
    "# This allows us to import our custom modules from the src/ folder.\n",
    "# We go up one level from notebooks/ to capstone_project/\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.insert(0, str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc337350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard data science imports\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1820655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings to keep output clean during training.\n",
    "# In production, you might want to log these instead.\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9780e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our custom modules from the src/ directory.\n",
    "# These are the same modules used in notebooks 01-04.\n",
    "from src.data.loader import load_and_merge_data, split_data_by_time\n",
    "from src.labeling.oracle import create_oracle_labels\n",
    "from src.features.builder import prepare_features\n",
    "from src.features.indicators import get_indicator_columns\n",
    "from src.models.xgb import XGBBaseline\n",
    "from src.models.cnn_lstm import CNNLSTMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c4a6577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸŽ“ TRAINING MODELS FOR HORIZONS 3 AND 5\n",
      "======================================================================\n",
      "\n",
      "This script will train XGBoost and CNN-LSTM models for two prediction horizons:\n",
      "  - Horizon 3 = predict direction 45 minutes ahead (3 Ã— 15-min bars)\n",
      "  - Horizon 5 = predict direction 75 minutes ahead (5 Ã— 15-min bars)\n",
      "\n",
      "Each horizon requires training 2 models = 4 models total.\n",
      "This will take approximately 30-60 minutes depending on your GPU.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ðŸŽ“ TRAINING MODELS FOR HORIZONS 3 AND 5\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "This script will train XGBoost and CNN-LSTM models for two prediction horizons:\n",
    "  - Horizon 3 = predict direction 45 minutes ahead (3 Ã— 15-min bars)\n",
    "  - Horizon 5 = predict direction 75 minutes ahead (5 Ã— 15-min bars)\n",
    "\n",
    "Each horizon requires training 2 models = 4 models total.\n",
    "This will take approximately 30-60 minutes depending on your GPU.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5376d282",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "STEP 1: CONFIGURATION\n",
    "==============================================================================\n",
    "\n",
    "All the key parameters are defined here in one place.\n",
    "This makes it easy to experiment with different settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1e8a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which horizons we want to train models for.\n",
    "# Horizon=1 is already done in notebook 02 and 03.\n",
    "# We need horizons 3 and 5 to complete the comparison.\n",
    "HORIZONS_TO_TRAIN = [3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a9f71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oracle label parameters - these MUST match what we used in notebooks 01-03.\n",
    "# If you use different values here, the models won't be comparable!\n",
    "SIGMA = 4           # Gaussian smoothing strength (higher = smoother trends)\n",
    "THRESHOLD = 0.0002  # Minimum slope to classify as UP or DOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edfc4982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split dates - MUST match the original training notebooks.\n",
    "# This ensures all models are evaluated on the exact same test data.\n",
    "TRAIN_END = \"2025-06-30\"    # Last date used for training\n",
    "TEST_START = \"2025-07-01\"   # First date used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1150e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory for saved models\n",
    "MODEL_DIR = 'models_artifacts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a511541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Configuration:\n",
      "   Horizons to train: [3, 5]\n",
      "   Oracle: sigma=4, threshold=0.0002\n",
      "   Train period: up to 2025-06-30\n",
      "   Test period: from 2025-07-01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“‹ Configuration:\")\n",
    "print(f\"   Horizons to train: {HORIZONS_TO_TRAIN}\")\n",
    "print(f\"   Oracle: sigma={SIGMA}, threshold={THRESHOLD}\")\n",
    "print(f\"   Train period: up to {TRAIN_END}\")\n",
    "print(f\"   Test period: from {TEST_START}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4256bc9",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "STEP 2: LOAD BASE DATA (DONE ONCE, USED FOR ALL HORIZONS)\n",
    "==============================================================================\n",
    "\n",
    "Loading and labeling data is the same regardless of the horizon.\n",
    "We do this ONCE here and then reuse for each horizon.\n",
    "This saves time compared to loading data inside the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bba5347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ“¥ STEP 2: LOADING AND LABELING DATA\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ðŸ“¥ STEP 2: LOADING AND LABELING DATA\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "965273c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  [1/2] Loading raw data from parquet files...\n",
      "\n",
      "============================================================\n",
      "ðŸ“¥ LOADING DATA\n",
      "============================================================\n",
      "\n",
      "1ï¸âƒ£ Loading klines...\n",
      "   Loaded 70,752 rows\n",
      "\n",
      "2ï¸âƒ£ Loading funding rates...\n",
      "   Merged funding rates\n",
      "\n",
      "3ï¸âƒ£ Loading volume breakdown...\n",
      "   Merged volume breakdown\n",
      "â° Filtered until: 2025-12-31\n",
      "\n",
      "ðŸ“Š Data Summary:\n",
      "   Rows: 70,081\n",
      "   Columns: 16\n",
      "   Date range: 2024-01-01 00:00:00+00:00 to 2025-12-31 00:00:00+00:00\n",
      "   Columns: ['time', 'open', 'high', 'low', 'close', 'volume', 'quote_volume', 'taker_buy_volume', 'taker_buy_quote_volume', 'count', 'funding_interval_hours', 'last_funding_rate', 'buy_vol', 'sell_vol', 'total_vol', 'log_return']\n",
      "        âœ“ Loaded 70,081 rows of 15-minute candles\n"
     ]
    }
   ],
   "source": [
    "# Load the raw data (OHLCV candles + funding rates + volume breakdown).\n",
    "# This combines data from multiple parquet files into one DataFrame.\n",
    "print(\"\\n  [1/2] Loading raw data from parquet files...\")\n",
    "df = load_and_merge_data(end_date='2025-12-31')\n",
    "print(f\"        âœ“ Loaded {len(df):,} rows of 15-minute candles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e899584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  [2/2] Creating oracle labels with Gaussian smoothing...\n",
      "        âœ“ Labels created with sigma=4, threshold=0.0002\n"
     ]
    }
   ],
   "source": [
    "# Create oracle labels (the target variable we're trying to predict).\n",
    "# Oracle uses Gaussian smoothing to create cleaner UP/DOWN/SIDEWAYS labels.\n",
    "print(\"\\n  [2/2] Creating oracle labels with Gaussian smoothing...\")\n",
    "df = create_oracle_labels(df, sigma=SIGMA, threshold=THRESHOLD)\n",
    "print(f\"        âœ“ Labels created with sigma={SIGMA}, threshold={THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a746fa95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Label distribution:\n",
      "        DOWN    :  29.3%\n",
      "        SIDEWAYS:  38.4%\n",
      "        UP      :  32.2%\n"
     ]
    }
   ],
   "source": [
    "# Show label distribution to verify it looks reasonable.\n",
    "label_dist = df['target'].value_counts(normalize=True).sort_index()\n",
    "label_names = {0: 'DOWN', 1: 'SIDEWAYS', 2: 'UP'}\n",
    "print(\"\\n  Label distribution:\")\n",
    "for label, pct in label_dist.items():\n",
    "    print(f\"        {label_names[label]:8s}: {pct*100:5.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8252dd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Data loading complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nâœ… Data loading complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4ed717",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "STEP 3: TRAIN MODELS FOR EACH HORIZON\n",
    "==============================================================================\n",
    "\n",
    "Now we loop through each horizon and train both XGBoost and CNN-LSTM.\n",
    "The key difference for each horizon is how far ahead we shift the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6d1a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dictionary will store results for summary at the end.\n",
    "all_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d9493d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸŽ¯ HORIZON 1/2: Training for horizon=3 (45 minutes ahead)\n",
      "======================================================================\n",
      "\n",
      "  ðŸ“Š Preparing features with horizon=3 shift...\n",
      "      This means: features at time T will predict direction at time T+3\n",
      "\n",
      "============================================================\n",
      "ðŸ”§ PREPARING FEATURES\n",
      "============================================================\n",
      "Groups to add: ['momentum', 'overlap', 'trend', 'volatility', 'volume', 'statistics', 'candle']\n",
      "\n",
      "ðŸ“Š Adding indicators...\n",
      "  -> Processing group: momentum\n",
      "     Added 14 features: MOM_10, ROC_12, WILLR_14, RSI_14, STOCHh_14_3_3...\n",
      "  -> Processing group: overlap\n",
      "[!] VWAP requires an ordered DatetimeIndex.\n",
      "     Added 7 features: HMA_9, SMA_200, EMA_20, TEMA_9, EMA_100...\n",
      "  -> Processing group: trend\n",
      "     Added 17 features: TRIXs_30_9, MACDs_12_26_9, TRIX_30_9, AROONU_14, DMN_14...\n",
      "  -> Processing group: volatility\n",
      "       BBL_20_2.0_2.0  BBM_20_2.0_2.0  BBU_20_2.0_2.0  BBB_20_2.0_2.0  \\\n",
      "0                 NaN             NaN             NaN             NaN   \n",
      "1                 NaN             NaN             NaN             NaN   \n",
      "2                 NaN             NaN             NaN             NaN   \n",
      "3                 NaN             NaN             NaN             NaN   \n",
      "4                 NaN             NaN             NaN             NaN   \n",
      "...               ...             ...             ...             ...   \n",
      "70076    87906.208343       88214.555    88522.901657        0.699083   \n",
      "70077    87889.395082       88224.055    88558.714918        0.758659   \n",
      "70078    87878.371545       88232.860    88587.348455        0.803529   \n",
      "70079    87874.507609       88242.645    88610.782391        0.834375   \n",
      "70080    87902.480057       88258.450    88614.419943        0.806654   \n",
      "\n",
      "       BBP_20_2.0_2.0  \n",
      "0                 NaN  \n",
      "1                 NaN  \n",
      "2                 NaN  \n",
      "3                 NaN  \n",
      "4                 NaN  \n",
      "...               ...  \n",
      "70076        0.811897  \n",
      "70077        0.950823  \n",
      "70078        0.877925  \n",
      "70079        0.788826  \n",
      "70080        0.617917  \n",
      "\n",
      "[70081 rows x 5 columns]\n",
      "          KCLe_20_2     KCBe_20_2     KCUe_20_2\n",
      "0               NaN           NaN           NaN\n",
      "1               NaN           NaN           NaN\n",
      "2               NaN           NaN           NaN\n",
      "3               NaN           NaN           NaN\n",
      "4               NaN           NaN           NaN\n",
      "...             ...           ...           ...\n",
      "70076  87899.333398  88275.506712  88651.680026\n",
      "70077  87928.711170  88299.344168  88669.977167\n",
      "70078  87971.119630  88318.530438  88665.941246\n",
      "70079  88002.317760  88331.556111  88660.794461\n",
      "70080  88005.392259  88332.588862  88659.785465\n",
      "\n",
      "[70081 rows x 3 columns]\n",
      "       DCL_20_20  DCM_20_20  DCU_20_20\n",
      "0            NaN        NaN        NaN\n",
      "1            NaN        NaN        NaN\n",
      "2            NaN        NaN        NaN\n",
      "3            NaN        NaN        NaN\n",
      "4            NaN        NaN        NaN\n",
      "...          ...        ...        ...\n",
      "70076    87833.2    88151.6    88470.0\n",
      "70077    87833.2    88183.1    88533.0\n",
      "70078    87833.2    88183.1    88533.0\n",
      "70079    87833.2    88183.1    88533.0\n",
      "70080    87833.2    88183.1    88533.0\n",
      "\n",
      "[70081 rows x 3 columns]\n",
      "     Added 5 features: NATR_14, UI_14, BBP_20_2.0_2.0, ATRr_14, BBB_20_2.0_2.0\n",
      "  -> Processing group: volume\n",
      "     Added 8 features: PVIe_255, EOM_14_100000000, MFI_14, PVI, AD...\n",
      "  -> Processing group: statistics\n",
      "     Added 6 features: MAD_30, ENTP_30, ZS_30, KURT_30, SKEW_30...\n",
      "  -> Processing group: candle\n",
      "     Added 0 features: \n",
      "\n",
      "â° Shifting target by -3 for prediction...\n",
      "\n",
      "ðŸ“‹ Top NaN columns:\n",
      "   PVIe_255: 254 NaN\n",
      "   SMA_200: 199 NaN\n",
      "   EMA_100: 99 NaN\n",
      "   ENTP_30: 58 NaN\n",
      "   EMA_50: 49 NaN\n",
      "\n",
      "ðŸ“‹ Warmup period: 254 rows, target shift: 1 row\n",
      "   Filling 19 remaining NaN with 0\n",
      "\n",
      "ðŸ§¹ Dropped 255 rows with NaN (69,826 remaining)\n",
      "\n",
      "ðŸ“Š Feature Summary:\n",
      "   Total rows: 69,826\n",
      "   Total columns: 76\n",
      "   Indicator groups:\n",
      "      momentum: 14 features\n",
      "      overlap: 7 features\n",
      "      trend: 17 features\n",
      "      volatility: 5 features\n",
      "      volume: 8 features\n",
      "      statistics: 6 features\n",
      "      candle: 0 features\n",
      "      other: 5 features\n",
      "      âœ“ Generated 62 features across 8 groups\n",
      "\n",
      "  ðŸ“Š Splitting data by time...\n",
      "\n",
      "ðŸ“Š Data Split:\n",
      "   Train: 46,947 rows (2024-01-03 15:30:00+00:00 to 2025-05-06 16:00:00+00:00)\n",
      "   Val:   5,216 rows (2025-05-06 16:15:00+00:00 to 2025-06-30 00:00:00+00:00)\n",
      "   Test:  17,568 rows (2025-07-01 00:00:00+00:00 to 2025-12-30 23:45:00+00:00)\n",
      "      âœ“ Training samples:   46,947\n",
      "      âœ“ Validation samples: 5,216\n",
      "      âœ“ Test samples:       17,568\n",
      "\n",
      "  ðŸ“Š Extracting feature matrices...\n",
      "      âœ“ Using 62 features for training\n",
      "      âœ“ Train shape: (46947, 62)\n",
      "      âœ“ Val shape:   (5216, 62)\n",
      "      âœ“ Test shape:  (17568, 62)\n",
      "\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  ðŸŒ² TRAINING XGBOOST (Model 1/2 for horizon=3)\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "      [1/3] Initializing XGBoost classifier...\n",
      "      [2/3] Training with early stopping...\n",
      "      [3/3] Tuning hyperparameters (this may take 5-10 minutes)...\n",
      "\n",
      "ðŸ”§ Tuning hyperparameters with TimeSeriesSplit...\n",
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n"
     ]
    }
   ],
   "source": [
    "# Process each horizon one at a time.\n",
    "for horizon_index, horizon in enumerate(HORIZONS_TO_TRAIN, start=1):\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"ðŸŽ¯ HORIZON {horizon_index}/{len(HORIZONS_TO_TRAIN)}: Training for horizon={horizon} ({horizon * 15} minutes ahead)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    all_results[horizon] = {}\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 3a: PREPARE FEATURES FOR THIS HORIZON\n",
    "    # =========================================================================\n",
    "    #\n",
    "    # The prepare_features() function does two things:\n",
    "    # 1. Adds 60+ technical indicators (RSI, MACD, Bollinger Bands, etc.)\n",
    "    # 2. Shifts the target column by 'horizon' rows to prevent data leakage\n",
    "    #\n",
    "    # The shift is crucial: when predicting horizon=3, we shift target by 3\n",
    "    # so that each row's features predict the direction 3 bars later.\n",
    "    \n",
    "    print(f\"\\n  ðŸ“Š Preparing features with horizon={horizon} shift...\")\n",
    "    print(f\"      This means: features at time T will predict direction at time T+{horizon}\")\n",
    "    \n",
    "    # IMPORTANT: We use df.copy() to avoid modifying the original DataFrame.\n",
    "    # Each horizon needs its own feature preparation because of different shifts.\n",
    "    df_features, group_map = prepare_features(df.copy(), horizon=horizon)\n",
    "    \n",
    "    # Count how many features we generated\n",
    "    total_features = sum(len(cols) for cols in group_map.values())\n",
    "    print(f\"      âœ“ Generated {total_features} features across {len(group_map)} groups\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 3b: SPLIT DATA INTO TRAIN / VALIDATION / TEST SETS\n",
    "    # =========================================================================\n",
    "    #\n",
    "    # For time series, we MUST use time-based splitting (not random!).\n",
    "    # The future data must never be seen during training.\n",
    "    #\n",
    "    # Timeline:\n",
    "    #   [Training Data]  |  [Validation]  |  [Test Data]\n",
    "    #   Jan 2024 ------> |  ~Jun 2025     |  Jul 2025 -->\n",
    "    \n",
    "    print(f\"\\n  ðŸ“Š Splitting data by time...\")\n",
    "    \n",
    "    train_df, val_df, test_df = split_data_by_time(\n",
    "        df_features,\n",
    "        train_end=TRAIN_END,\n",
    "        test_start=TEST_START,\n",
    "        val_ratio=0.1  # Last 10% of training period is used for validation\n",
    "    )\n",
    "    \n",
    "    print(f\"      âœ“ Training samples:   {len(train_df):,}\")\n",
    "    print(f\"      âœ“ Validation samples: {len(val_df):,}\")\n",
    "    print(f\"      âœ“ Test samples:       {len(test_df):,}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 3c: EXTRACT FEATURE COLUMNS AND CONVERT TO NUMPY ARRAYS\n",
    "    # =========================================================================\n",
    "    #\n",
    "    # We need to extract just the feature columns (not target, not metadata).\n",
    "    # Then convert to numpy arrays for scikit-learn and TensorFlow.\n",
    "    \n",
    "    print(f\"\\n  ðŸ“Š Extracting feature matrices...\")\n",
    "    \n",
    "    # Get list of feature column names (excludes target and helper columns)\n",
    "    feature_cols = get_indicator_columns(\n",
    "        df_features, \n",
    "        exclude_cols=['time', 'target', 'smoothed_close', 'smooth_slope']\n",
    "    )\n",
    "    feature_cols = [c for c in feature_cols if c in train_df.columns]\n",
    "    \n",
    "    print(f\"      âœ“ Using {len(feature_cols)} features for training\")\n",
    "    \n",
    "    # Convert to numpy arrays for the models.\n",
    "    # nan_to_num replaces NaN, +inf, -inf with 0 to prevent training errors.\n",
    "    X_train = np.nan_to_num(train_df[feature_cols].values, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    y_train = train_df['target'].values.astype(int)\n",
    "    \n",
    "    X_val = np.nan_to_num(val_df[feature_cols].values, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    y_val = val_df['target'].values.astype(int)\n",
    "    \n",
    "    X_test = np.nan_to_num(test_df[feature_cols].values, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    y_test = test_df['target'].values.astype(int)\n",
    "    \n",
    "    print(f\"      âœ“ Train shape: {X_train.shape}\")\n",
    "    print(f\"      âœ“ Val shape:   {X_val.shape}\")\n",
    "    print(f\"      âœ“ Test shape:  {X_test.shape}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 3d: TRAIN XGBOOST MODEL\n",
    "    # =========================================================================\n",
    "    #\n",
    "    # XGBoost is our baseline model. It's fast, interpretable, and works\n",
    "    # well on tabular data. We use GPU acceleration for faster training.\n",
    "    \n",
    "    print(f\"\\n  {'â”€' * 60}\")\n",
    "    print(f\"  ðŸŒ² TRAINING XGBOOST (Model 1/2 for horizon={horizon})\")\n",
    "    print(f\"  {'â”€' * 60}\")\n",
    "    \n",
    "    # Create the XGBoost model wrapper.\n",
    "    # n_classes=3 because we have 3 labels: DOWN(0), SIDEWAYS(1), UP(2)\n",
    "    # device='cuda' enables GPU training which is much faster\n",
    "    print(f\"\\n      [1/3] Initializing XGBoost classifier...\")\n",
    "    xgb_model = XGBBaseline(\n",
    "        n_classes=3,        # 3-class classification\n",
    "        device='cuda',      # Use GPU for training\n",
    "        random_state=42     # For reproducibility\n",
    "    )\n",
    "    \n",
    "    # Train the model on training data with validation for early stopping.\n",
    "    # Early stopping prevents overfitting by stopping when validation score stops improving.\n",
    "    print(f\"      [2/3] Training with early stopping...\")\n",
    "    xgb_model.fit(\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        feature_names=feature_cols  # Useful for feature importance later\n",
    "    )\n",
    "    \n",
    "    # Hyperparameter tuning with RandomizedSearchCV.\n",
    "    # This tries different combinations of parameters to find the best ones.\n",
    "    # We use fewer iterations here (15 instead of 25) for faster execution.\n",
    "    print(f\"      [3/3] Tuning hyperparameters (this may take 5-10 minutes)...\")\n",
    "    best_params = xgb_model.tune(\n",
    "        X_train, y_train,\n",
    "        n_iter=15,              # Try 15 random parameter combinations\n",
    "        cv_splits=3,            # 3-fold time series cross-validation\n",
    "        scoring='f1_weighted'   # Optimize for weighted F1 score\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n      Best parameters found:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"        {param}: {value}\")\n",
    "    \n",
    "    # Evaluate on the held-out test set.\n",
    "    # This gives us an unbiased estimate of real-world performance.\n",
    "    xgb_metrics = xgb_model.evaluate(X_test, y_test)\n",
    "    \n",
    "    print(f\"\\n      âœ… XGBoost Results (horizon={horizon}):\")\n",
    "    print(f\"         Accuracy:    {xgb_metrics['accuracy']:.4f} ({xgb_metrics['accuracy']:.1%})\")\n",
    "    print(f\"         F1 Weighted: {xgb_metrics['f1_weighted']:.4f}\")\n",
    "    print(f\"         F1 Macro:    {xgb_metrics['f1_macro']:.4f}\")\n",
    "    \n",
    "    # Save the trained model to disk.\n",
    "    Path(MODEL_DIR).mkdir(exist_ok=True)\n",
    "    xgb_model.save(MODEL_DIR, name=f'xgb_baseline_h{horizon}')\n",
    "    print(f\"\\n      ðŸ’¾ Model saved: {MODEL_DIR}/xgb_baseline_h{horizon}_model.joblib\")\n",
    "    \n",
    "    # Store results for summary table\n",
    "    all_results[horizon]['XGBoost'] = xgb_metrics\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 3e: TRAIN CNN-LSTM MODEL\n",
    "    # =========================================================================\n",
    "    #\n",
    "    # CNN-LSTM is our advanced deep learning model. It can capture both\n",
    "    # local patterns (CNN) and temporal dependencies (LSTM).\n",
    "    #\n",
    "    # CNN = Convolutional Neural Network - good at finding local patterns\n",
    "    # LSTM = Long Short-Term Memory - good at remembering past information\n",
    "    \n",
    "    print(f\"\\n  {'â”€' * 60}\")\n",
    "    print(f\"  ðŸ§  TRAINING CNN-LSTM (Model 2/2 for horizon={horizon})\")\n",
    "    print(f\"  {'â”€' * 60}\")\n",
    "    \n",
    "    # Create the CNN-LSTM model.\n",
    "    # These hyperparameters were tuned in notebook 03 for horizon=1.\n",
    "    # We reuse them here as a reasonable starting point.\n",
    "    print(f\"\\n      [1/2] Initializing CNN-LSTM model...\")\n",
    "    cnn_model = CNNLSTMModel(\n",
    "        n_classes=3,                    # 3-class classification\n",
    "        lookback=32,                    # Look at 32 time steps of history\n",
    "        conv_filters=64,                # 64 convolutional filters\n",
    "        lstm_units=64,                  # 64 LSTM units\n",
    "        dropout=0.3,                    # 30% dropout to prevent overfitting\n",
    "        learning_rate=0.0007,           # Adam learning rate (reduced 30% from 0.001 for stability)\n",
    "        device='cuda'                   # Use GPU for training\n",
    "    )\n",
    "    \n",
    "    # Train the model.\n",
    "    # IMPORTANT: We use aggressive early stopping to prevent overfitting.\n",
    "    # Based on observed training behavior:\n",
    "    #   - Accuracy typically peaks in first 3-5 epochs\n",
    "    #   - Then slowly decreases over next 10+ epochs if we keep training\n",
    "    #   - This is classic overfitting - model memorizes training data\n",
    "    #\n",
    "    # Our solution:\n",
    "    #   - patience=5: Stop if no improvement for 5 epochs (was 10)\n",
    "    #   - epochs=30: Maximum epochs (was 50), but early stopping usually kicks in earlier\n",
    "    #   - batch_size=128: Good balance between speed and gradient quality\n",
    "    print(f\"      [2/2] Training neural network...\")\n",
    "    print(f\"           Using aggressive early stopping (patience=5) to prevent overfitting\")\n",
    "    print(f\"           Watch for training progress below...\")\n",
    "    \n",
    "    history = cnn_model.fit(\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        epochs=30,                      # Maximum epochs (reduced from 50)\n",
    "        batch_size=128,                 # Samples per gradient update\n",
    "        early_stopping_patience=5       # Stop if no improvement for 5 epochs (reduced from 10)\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    cnn_metrics = cnn_model.evaluate(X_test, y_test)\n",
    "    \n",
    "    print(f\"\\n      âœ… CNN-LSTM Results (horizon={horizon}):\")\n",
    "    print(f\"         Accuracy:    {cnn_metrics['accuracy']:.4f} ({cnn_metrics['accuracy']:.1%})\")\n",
    "    print(f\"         F1 Weighted: {cnn_metrics['f1_weighted']:.4f}\")\n",
    "    print(f\"         F1 Macro:    {cnn_metrics['f1_macro']:.4f}\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    cnn_model.save(MODEL_DIR, name=f'cnn_lstm_h{horizon}')\n",
    "    print(f\"\\n      ðŸ’¾ Model saved: {MODEL_DIR}/cnn_lstm_h{horizon}_model.keras\")\n",
    "    \n",
    "    # Store results\n",
    "    all_results[horizon]['CNN-LSTM'] = cnn_metrics\n",
    "    \n",
    "    print(f\"\\n  âœ… Horizon={horizon} complete! Both models trained and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ae5535",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "STEP 4: FINAL SUMMARY\n",
    "==============================================================================\n",
    "\n",
    "Print a summary of all trained models so you can see results at a glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4921881",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸ“‹ TRAINING COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527559bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "{'Horizon':<12} {'Model':<12} {'Accuracy':<12} {'F1 Weighted':<12}\n",
    "{'â”€' * 48}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2153f94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for horizon in HORIZONS_TO_TRAIN:\n",
    "    for model_name in ['XGBoost', 'CNN-LSTM']:\n",
    "        metrics = all_results[horizon][model_name]\n",
    "        print(f\"{horizon:<12} {model_name:<12} {metrics['accuracy']:.4f}       {metrics['f1_weighted']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394c755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "{'â”€' * 48}\n",
    "\n",
    "ðŸ“ Models saved to: {MODEL_DIR}/\n",
    "   - xgb_baseline_h3_model.joblib\n",
    "   - xgb_baseline_h5_model.joblib\n",
    "   - cnn_lstm_h3_model.keras\n",
    "   - cnn_lstm_h5_model.keras\n",
    "\n",
    "ðŸ”œ NEXT STEP:\n",
    "   Re-run notebook 04_comparison.ipynb to see the full comparison\n",
    "   across all horizons (1, 3, and 5).\n",
    "\n",
    "âœ… ALL DONE!\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
