{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f93597",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==============================================================================\n",
    "03b - TCN-ATTENTION HYPERPARAMETER TUNING (Grid Search)\n",
    "==============================================================================\n",
    "\n",
    "This script runs a randomized grid search over key TCN-Attention hyperparameters\n",
    "to find the best configuration for our Bitcoin price direction task.\n",
    "\n",
    "Search axes (based on expert recommendations):\n",
    "- tcn_filters: 32, 64, 128 (model capacity)\n",
    "- num_tcn_blocks: 2, 3, 4 (network depth)  \n",
    "- lookback: 16, 32, 64 (how much history to consider)\n",
    "- dropout: 0.1, 0.2, 0.3 (regularization strength)\n",
    "- use_class_weights: True, False (handle class imbalance)\n",
    "\n",
    "We sample 20 random combinations from 162 total to keep runtime reasonable.\n",
    "Each config takes ~2-3 minutes, so full search is ~40-60 min.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path(__file__).resolve().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import random\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src.data.loader import load_and_merge_data, split_data_by_time\n",
    "from src.labeling.oracle import create_oracle_labels\n",
    "from src.features.builder import prepare_features\n",
    "from src.features.indicators import get_indicator_columns\n",
    "from src.models.tcn_attention import TCNAttentionModel\n",
    "\n",
    "print(\"‚úÖ Imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4321459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# Oracle labeling params - must match other notebooks for fair comparison\n",
    "SIGMA = 3\n",
    "THRESHOLD = 0.0002\n",
    "HORIZON = 1\n",
    "TRAIN_END = \"2025-06-30\"\n",
    "TEST_START = \"2025-07-01\"\n",
    "MODEL_DIR = 'models_artifacts'\n",
    "\n",
    "# Hyperparameter search space\n",
    "PARAM_GRID = {\n",
    "    'tcn_filters': [32, 64, 128],\n",
    "    'num_tcn_blocks': [2, 3, 4],\n",
    "    'lookback': [16, 32, 64],\n",
    "    'dropout': [0.1, 0.2, 0.3],\n",
    "    'use_class_weights': [True, False]\n",
    "}\n",
    "\n",
    "# How many random combinations to try (full grid = 162 combos)\n",
    "N_RANDOM_SAMPLES = 20\n",
    "\n",
    "# Fixed params that we don't search over\n",
    "FIXED_PARAMS = {\n",
    "    'kernel_size': 3,\n",
    "    'attention_heads': 4,\n",
    "    'dense_units': 32,\n",
    "    'learning_rate': 0.0007,\n",
    "    'epochs': 30,\n",
    "    'patience': 5,\n",
    "    'batch_size': 128\n",
    "}\n",
    "\n",
    "print(\"üìã Grid Search Configuration:\")\n",
    "print(f\"   Random samples: {N_RANDOM_SAMPLES}\")\n",
    "print(f\"   Parameter grid: {list(PARAM_GRID.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0891625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LOAD DATA\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üì• LOADING DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"   Loading raw OHLCV data...\")\n",
    "df = load_and_merge_data(end_date='2025-12-31')\n",
    "print(f\"   ‚úì Loaded {len(df):,} candles\")\n",
    "\n",
    "print(\"   Creating oracle labels...\")\n",
    "df = create_oracle_labels(df, sigma=SIGMA, threshold=THRESHOLD)\n",
    "\n",
    "print(\"   Building feature matrix...\")\n",
    "df_features, _ = prepare_features(df, horizon=HORIZON)\n",
    "\n",
    "print(\"   Splitting into train/val/test...\")\n",
    "train_df, val_df, test_df = split_data_by_time(\n",
    "    df_features, train_end=TRAIN_END, test_start=TEST_START, val_ratio=0.1\n",
    ")\n",
    "\n",
    "# Get feature column names (exclude target and metadata)\n",
    "feature_cols = get_indicator_columns(\n",
    "    df_features, exclude_cols=['time', 'target', 'smoothed_close', 'smooth_slope']\n",
    ")\n",
    "feature_cols = [c for c in feature_cols if c in train_df.columns]\n",
    "\n",
    "# Convert to numpy, replacing any NaN/inf with 0\n",
    "X_train = np.nan_to_num(train_df[feature_cols].values, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "y_train = train_df['target'].values.astype(int)\n",
    "X_val = np.nan_to_num(val_df[feature_cols].values, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "y_val = val_df['target'].values.astype(int)\n",
    "X_test = np.nan_to_num(test_df[feature_cols].values, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "y_test = test_df['target'].values.astype(int)\n",
    "\n",
    "print(f\"\\n   Train: {X_train.shape[0]:,} samples\")\n",
    "print(f\"   Val:   {X_val.shape[0]:,} samples\")\n",
    "print(f\"   Test:  {X_test.shape[0]:,} samples\")\n",
    "print(f\"   Features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da0f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# GENERATE RANDOM COMBINATIONS\n",
    "# ==============================================================================\n",
    "\n",
    "# Generate all possible parameter combinations\n",
    "all_combinations = list(itertools.product(*PARAM_GRID.values()))\n",
    "param_names = list(PARAM_GRID.keys())\n",
    "\n",
    "print(f\"\\nüìä Total possible combinations: {len(all_combinations)}\")\n",
    "print(f\"   Sampling {N_RANDOM_SAMPLES} random combinations...\")\n",
    "\n",
    "# Random sample for efficiency (full grid would take too long)\n",
    "random.seed(42)\n",
    "sampled_indices = random.sample(range(len(all_combinations)), min(N_RANDOM_SAMPLES, len(all_combinations)))\n",
    "sampled_combinations = [all_combinations[i] for i in sampled_indices]\n",
    "\n",
    "# Convert tuples to dicts for easier handling\n",
    "param_configs = []\n",
    "for combo in sampled_combinations:\n",
    "    config = dict(zip(param_names, combo))\n",
    "    param_configs.append(config)\n",
    "\n",
    "print(f\"   ‚úì Selected {len(param_configs)} configs to test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a51289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# RUN GRID SEARCH\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîç RUNNING GRID SEARCH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, params in enumerate(param_configs):\n",
    "    print(f\"\\n{'‚îÄ'*60}\")\n",
    "    print(f\"[{i+1}/{len(param_configs)}] Testing config:\")\n",
    "    print(f\"   filters={params['tcn_filters']}, blocks={params['num_tcn_blocks']}, \"\n",
    "          f\"lookback={params['lookback']}, dropout={params['dropout']}, \"\n",
    "          f\"class_weights={params['use_class_weights']}\")\n",
    "    print(\"‚îÄ\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Create model with current hyperparameters\n",
    "        print(\"   Creating model...\")\n",
    "        model = TCNAttentionModel(\n",
    "            n_classes=3,\n",
    "            lookback=params['lookback'],\n",
    "            tcn_filters=params['tcn_filters'],\n",
    "            num_tcn_blocks=params['num_tcn_blocks'],\n",
    "            dropout=params['dropout'],\n",
    "            kernel_size=FIXED_PARAMS['kernel_size'],\n",
    "            attention_heads=FIXED_PARAMS['attention_heads'],\n",
    "            dense_units=FIXED_PARAMS['dense_units'],\n",
    "            learning_rate=FIXED_PARAMS['learning_rate'],\n",
    "            device='cuda'\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        print(\"   Training...\")\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            X_val, y_val,\n",
    "            epochs=FIXED_PARAMS['epochs'],\n",
    "            batch_size=FIXED_PARAMS['batch_size'],\n",
    "            patience=FIXED_PARAMS['patience'],\n",
    "            use_class_weights=params['use_class_weights']\n",
    "        )\n",
    "        \n",
    "        # Evaluate on validation set (not test - save that for final model)\n",
    "        print(\"   Evaluating on validation set...\")\n",
    "        val_metrics = model.evaluate(X_val, y_val)\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            **params,\n",
    "            'val_accuracy': val_metrics['accuracy'],\n",
    "            'val_f1_weighted': val_metrics['f1_weighted'],\n",
    "            'val_f1_macro': val_metrics['f1_macro']\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"   ‚úÖ val_acc={val_metrics['accuracy']:.4f}, val_f1={val_metrics['f1_weighted']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "        results.append({**params, 'val_accuracy': 0, 'val_f1_weighted': 0, 'val_f1_macro': 0, 'error': str(e)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e9c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ANALYZE RESULTS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä GRID SEARCH RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sort by validation F1 score (our main metric)\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('val_f1_weighted', ascending=False)\n",
    "\n",
    "print(\"\\nüèÜ TOP 5 CONFIGURATIONS (by val_f1_weighted):\\n\")\n",
    "top5 = results_df.head(5)\n",
    "for idx, row in top5.iterrows():\n",
    "    print(f\"  F1={row['val_f1_weighted']:.4f} | \"\n",
    "          f\"filters={int(row['tcn_filters'])}, blocks={int(row['num_tcn_blocks'])}, \"\n",
    "          f\"lookback={int(row['lookback'])}, dropout={row['dropout']}, \"\n",
    "          f\"class_weights={row['use_class_weights']}\")\n",
    "\n",
    "# Extract best params\n",
    "best_params = results_df.iloc[0].to_dict()\n",
    "print(f\"\\nü•á BEST CONFIGURATION:\")\n",
    "print(f\"   tcn_filters:      {int(best_params['tcn_filters'])}\")\n",
    "print(f\"   num_tcn_blocks:   {int(best_params['num_tcn_blocks'])}\")\n",
    "print(f\"   lookback:         {int(best_params['lookback'])}\")\n",
    "print(f\"   dropout:          {best_params['dropout']}\")\n",
    "print(f\"   use_class_weights: {best_params['use_class_weights']}\")\n",
    "print(f\"   val_f1_weighted:  {best_params['val_f1_weighted']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d552b324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TRAIN FINAL MODEL WITH BEST PARAMS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÜ TRAINING FINAL MODEL WITH BEST PARAMS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"   Creating model with best hyperparameters...\")\n",
    "best_model = TCNAttentionModel(\n",
    "    n_classes=3,\n",
    "    lookback=int(best_params['lookback']),\n",
    "    tcn_filters=int(best_params['tcn_filters']),\n",
    "    num_tcn_blocks=int(best_params['num_tcn_blocks']),\n",
    "    dropout=best_params['dropout'],\n",
    "    kernel_size=FIXED_PARAMS['kernel_size'],\n",
    "    attention_heads=FIXED_PARAMS['attention_heads'],\n",
    "    dense_units=FIXED_PARAMS['dense_units'],\n",
    "    learning_rate=FIXED_PARAMS['learning_rate'],\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "print(\"   Training final model...\")\n",
    "best_model.fit(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    epochs=FIXED_PARAMS['epochs'],\n",
    "    batch_size=FIXED_PARAMS['batch_size'],\n",
    "    patience=FIXED_PARAMS['patience'],\n",
    "    use_class_weights=best_params['use_class_weights']\n",
    ")\n",
    "\n",
    "# Now we can evaluate on the held-out test set\n",
    "print(\"   Evaluating on test set (held out until now)...\")\n",
    "test_metrics = best_model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"\\n‚úÖ FINAL TEST SET RESULTS:\")\n",
    "print(f\"   Accuracy:    {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"   F1 Weighted: {test_metrics['f1_weighted']:.4f}\")\n",
    "print(f\"   F1 Macro:    {test_metrics['f1_macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f683707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SAVE RESULTS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üíæ SAVING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save the trained model\n",
    "print(\"   Saving model weights...\")\n",
    "Path(MODEL_DIR).mkdir(exist_ok=True)\n",
    "best_model.save(MODEL_DIR, name=f'tcn_attention_h{HORIZON}')\n",
    "\n",
    "# Save grid search results as CSV for later analysis\n",
    "print(\"   Saving grid search results...\")\n",
    "results_df.to_csv(f'{MODEL_DIR}/tcn_attention_grid_search_results.csv', index=False)\n",
    "\n",
    "# Save best params as JSON for notebook 04 to load\n",
    "print(\"   Saving best parameters...\")\n",
    "best_params_clean = {\n",
    "    'tcn_filters': int(best_params['tcn_filters']),\n",
    "    'num_tcn_blocks': int(best_params['num_tcn_blocks']),\n",
    "    'lookback': int(best_params['lookback']),\n",
    "    'dropout': float(best_params['dropout']),\n",
    "    'use_class_weights': bool(best_params['use_class_weights']),\n",
    "    'val_f1_weighted': float(best_params['val_f1_weighted']),\n",
    "    'test_accuracy': float(test_metrics['accuracy']),\n",
    "    'test_f1_weighted': float(test_metrics['f1_weighted'])\n",
    "}\n",
    "\n",
    "with open(f'{MODEL_DIR}/tcn_attention_best_params.json', 'w') as f:\n",
    "    json.dump(best_params_clean, f, indent=2)\n",
    "\n",
    "print(f\"\\n   ‚úÖ Model: {MODEL_DIR}/tcn_attention_h{HORIZON}_model.keras\")\n",
    "print(f\"   ‚úÖ Grid results: {MODEL_DIR}/tcn_attention_grid_search_results.csv\")\n",
    "print(f\"   ‚úÖ Best params: {MODEL_DIR}/tcn_attention_best_params.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed9879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SUMMARY\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "Grid Search Complete!\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "‚Ä¢ Tested {len(results)} configurations\n",
    "‚Ä¢ Best val F1 weighted: {best_params['val_f1_weighted']:.4f}\n",
    "\n",
    "Best Hyperparameters:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "‚Ä¢ tcn_filters:      {int(best_params['tcn_filters'])}\n",
    "‚Ä¢ num_tcn_blocks:   {int(best_params['num_tcn_blocks'])}\n",
    "‚Ä¢ lookback:         {int(best_params['lookback'])}\n",
    "‚Ä¢ dropout:          {best_params['dropout']}\n",
    "‚Ä¢ use_class_weights: {best_params['use_class_weights']}\n",
    "\n",
    "Final Test Results:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "‚Ä¢ Accuracy:    {test_metrics['accuracy']:.4f}\n",
    "‚Ä¢ F1 Weighted: {test_metrics['f1_weighted']:.4f}\n",
    "‚Ä¢ F1 Macro:    {test_metrics['f1_macro']:.4f}\n",
    "\n",
    "Next: Run 05_comparison to compare with XGBoost and CNN-LSTM.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
