{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616bdfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==============================================================================\n",
    "03 - CNN-LSTM ADVANCED MODEL\n",
    "==============================================================================\n",
    "\n",
    "PURPOSE OF THIS NOTEBOOK:\n",
    "-------------------------\n",
    "Train and evaluate a CNN-LSTM neural network for price direction prediction.\n",
    "\n",
    "This is our ADVANCED model because:\n",
    "1. CNN layers capture local patterns (like candlestick patterns)\n",
    "2. LSTM layers capture temporal dependencies (memory of past states)\n",
    "3. Combination is powerful for sequential financial data\n",
    "\n",
    "WHY CNN + LSTM?\n",
    "---------------\n",
    "- CNN: Good at finding local patterns in data (like chart patterns)\n",
    "- LSTM: Good at remembering long-term dependencies\n",
    "- Together: Can learn complex temporal patterns in price movements\n",
    "\n",
    "EXPECTED RESULTS:\n",
    "-----------------\n",
    "- Neural networks often match or slightly beat tree-based models\n",
    "- Main advantage: Can learn non-linear patterns automatically\n",
    "- Main disadvantage: Slower training, needs more data\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeaaf02",
   "metadata": {},
   "source": [
    "# CNN-LSTM Advanced Model\n",
    "\n",
    "**Goal**: Train a deep learning model that can capture temporal patterns.\n",
    "\n",
    "**Architecture**:\n",
    "1. Input: Sequence of N past feature vectors (lookback window)\n",
    "2. Conv1D: Extract local patterns\n",
    "3. LSTM: Capture temporal dependencies\n",
    "4. Dense: Classification into UP/SIDEWAYS/DOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0042f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# IMPORTS AND GPU CHECK\n",
    "# ==============================================================================\n",
    "#\n",
    "# TensorFlow/Keras is used for the neural network.\n",
    "# We check for GPU availability because training is MUCH faster on GPU.\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path(__file__).resolve().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# ==============================================================================\n",
    "# GPU AVAILABILITY CHECK\n",
    "# ==============================================================================\n",
    "# Neural networks are 10-100x faster on GPU compared to CPU.\n",
    "# If no GPU is detected, training will still work but be slower.\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ–¥ï¸ SYSTEM CHECK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"âœ… GPU available: {gpus[0].name}\")\n",
    "    print(\"   Training will be fast!\")\n",
    "else:\n",
    "    print(\"âš ï¸ No GPU detected - training will be slower\")\n",
    "    print(\"   Consider using Google Colab for faster training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a948d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# IMPORT PROJECT MODULES\n",
    "# ==============================================================================\n",
    "\n",
    "# Reload modules to pick up any code changes without restarting kernel\n",
    "import importlib\n",
    "import src.data.loader\n",
    "import src.labeling.oracle\n",
    "import src.features.builder\n",
    "import src.features.indicators\n",
    "import src.models.cnn_lstm\n",
    "importlib.reload(src.data.loader)\n",
    "importlib.reload(src.labeling.oracle)\n",
    "importlib.reload(src.features.builder)\n",
    "importlib.reload(src.features.indicators)\n",
    "importlib.reload(src.models.cnn_lstm)\n",
    "\n",
    "from src.data.loader import load_and_merge_data, split_data_by_time\n",
    "from src.labeling.oracle import create_oracle_labels\n",
    "from src.features.builder import prepare_features\n",
    "from src.features.indicators import get_indicator_columns\n",
    "from src.models.cnn_lstm import CNNLSTMModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5add7d",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "**NEW PARAMETER - LOOKBACK:**\n",
    "\n",
    "Unlike XGBoost which sees one row at a time, CNN-LSTM sees a SEQUENCE of rows.\n",
    "`LOOKBACK=20` means the model sees the last 20 candles (5 hours for 15-min data).\n",
    "\n",
    "This allows the model to learn patterns like:\n",
    "- \"Price usually reverses after 3 consecutive red candles\"\n",
    "- \"High volume followed by low volume often precedes breakouts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42109ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CONFIGURATION PARAMETERS\n",
    "# ==============================================================================\n",
    "\n",
    "# Oracle label parameters (same as baseline for fair comparison)\n",
    "SIGMA = 3           # Gaussian smoothing sigma\n",
    "THRESHOLD = 0.0002  # Slope threshold for direction classification\n",
    "\n",
    "# Prediction horizon\n",
    "HORIZON = 1         # Predict next bar direction\n",
    "\n",
    "# CNN-LSTM specific parameter\n",
    "LOOKBACK = 10       # How many past candles to look at (10 showed better results than 20)\n",
    "                    # 20 candles Ã— 15 min = 5 hours of history\n",
    "                    # Experiment with: 5, 10, 20, 30, 50\n",
    "\n",
    "# Train/Test split dates\n",
    "TRAIN_END = \"2025-06-30\"\n",
    "TEST_START = \"2025-07-01\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“‹ CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Oracle: sigma={SIGMA}, threshold={THRESHOLD}\")\n",
    "print(f\"Prediction Horizon: {HORIZON} bar(s)\")\n",
    "print(f\"Lookback Window: {LOOKBACK} candles ({LOOKBACK * 15} minutes)\")\n",
    "print(f\"Training: up to {TRAIN_END}\")\n",
    "print(f\"Testing: from {TEST_START}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ceeffa",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data\n",
    "\n",
    "Same data loading as baseline for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf3ea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LOAD DATA AND CREATE LABELS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“¥ LOADING DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df = load_and_merge_data(end_date='2025-12-31')\n",
    "df = create_oracle_labels(df, sigma=SIGMA, threshold=THRESHOLD)\n",
    "\n",
    "print(f\"\\nâœ… Loaded {len(df):,} rows\")\n",
    "print(\"\\nLabel distribution:\")\n",
    "label_counts = df['target'].value_counts(normalize=True).sort_index()\n",
    "label_names = {0: 'DOWN', 1: 'SIDEWAYS', 2: 'UP'}\n",
    "for label, pct in label_counts.items():\n",
    "    print(f\"  {label_names[label]:8s}: {pct*100:5.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010760c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# GENERATE FEATURES\n",
    "# ==============================================================================\n",
    "#\n",
    "# For neural networks, we use fewer feature groups to avoid overfitting.\n",
    "# Deep learning can learn features automatically, so we don't need as many.\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âš™ï¸ GENERATING FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use fewer groups for neural network (less prone to overfitting)\n",
    "feature_groups = ['momentum', 'overlap', 'trend', 'volatility', 'volume', 'statistics']\n",
    "df_features, group_map = prepare_features(df, groups=feature_groups, horizon=HORIZON)\n",
    "\n",
    "print(\"\\nFeature groups used:\")\n",
    "for group, cols in group_map.items():\n",
    "    print(f\"  {group}: {len(cols)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932e03a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TIME-BASED SPLIT\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š SPLITTING DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_df, val_df, test_df = split_data_by_time(\n",
    "    df_features,\n",
    "    train_end=TRAIN_END,\n",
    "    test_start=TEST_START,\n",
    "    val_ratio=0.1  # Same as XGB for fair comparison\n",
    ")\n",
    "\n",
    "feature_cols = get_indicator_columns(\n",
    "    df_features, \n",
    "    exclude_cols=['time', 'target', 'smoothed_close', 'smooth_slope']\n",
    ")\n",
    "feature_cols = [c for c in feature_cols if c in train_df.columns]\n",
    "print(f\"\\nâœ… Features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0126a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EXTRACT FEATURE MATRICES\n",
    "# ==============================================================================\n",
    "#\n",
    "# Note: The CNN-LSTM model will internally convert these 2D arrays\n",
    "# to 3D sequences using the LOOKBACK parameter.\n",
    "# Shape: (samples, features) â†’ (samples, lookback, features)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“ EXTRACTING FEATURE MATRICES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_train = train_df[feature_cols].values\n",
    "y_train = train_df['target'].values.astype(int)\n",
    "\n",
    "X_val = val_df[feature_cols].values\n",
    "y_val = val_df['target'].values.astype(int)\n",
    "\n",
    "X_test = test_df[feature_cols].values\n",
    "y_test = test_df['target'].values.astype(int)\n",
    "\n",
    "# Clean inf/nan values (some indicators like EOM produce inf from division by zero)\n",
    "X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "X_val = np.nan_to_num(X_val, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "X_test = np.nan_to_num(X_test, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "print(f\"Training:   {X_train.shape[0]:,} samples Ã— {X_train.shape[1]} features\")\n",
    "print(f\"Validation: {X_val.shape[0]:,} samples Ã— {X_val.shape[1]} features\")\n",
    "print(f\"Test:       {X_test.shape[0]:,} samples Ã— {X_test.shape[1]} features\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ After sequence creation (lookback={LOOKBACK}):\")\n",
    "print(f\"   Input shape will be: (samples, {LOOKBACK}, {X_train.shape[1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40781846",
   "metadata": {},
   "source": [
    "## 3. Train CNN-LSTM Model\n",
    "\n",
    "**MODEL ARCHITECTURE:**\n",
    "\n",
    "```\n",
    "Input (lookback Ã— features)\n",
    "    â†“\n",
    "Conv1D (extract local patterns)\n",
    "    â†“\n",
    "MaxPooling (reduce dimensionality)\n",
    "    â†“\n",
    "LSTM (capture temporal dependencies)\n",
    "    â†“\n",
    "Dropout (prevent overfitting)\n",
    "    â†“\n",
    "Dense (classification)\n",
    "    â†“\n",
    "Output (3 classes)\n",
    "```\n",
    "\n",
    "**Key Parameters:**\n",
    "- `conv_filters`: Number of 1D convolution filters\n",
    "- `lstm_units`: LSTM hidden state size\n",
    "- `dropout`: Fraction of units to drop (regularization)\n",
    "- `learning_rate`: Step size for optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011e6679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# HYPERPARAMETER TUNING (GRID SEARCH)\n",
    "# ==============================================================================\n",
    "#\n",
    "# ĞŸĞµÑ€ĞµĞ±Ğ¸Ñ€Ğ°ĞµĞ¼ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ½Ğ°Ñ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸.\n",
    "# ĞšĞ°Ğ¶Ğ´Ğ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡Ğ°ĞµÑ‚ÑÑ Ñ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ ÑĞ¿Ğ¾Ñ… Ğ´Ğ»Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸.\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ”§ HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ±Ğ¾Ñ€Ğ°\n",
    "PARAM_GRID = {\n",
    "    'conv_filters': [16,32, 64, 128],           # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ñ‹ Conv1D\n",
    "    'dropout': [0.1, 0.2, 0.3],        # Dropout rate\n",
    "    'learning_rate': [0.001,],   # Learning rate\n",
    "    'lstm_units': [64, 96, 128],             # LSTM units\n",
    "    'batch_size': [32, 64],             # Batch size\n",
    "}\n",
    "\n",
    "# Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ ÑĞ¿Ğ¾Ñ…\n",
    "SEARCH_EPOCHS = 20\n",
    "SEARCH_PATIENCE = 7\n",
    "\n",
    "print(f\"\\nĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ±Ğ¾Ñ€Ğ°:\")\n",
    "for param, values in PARAM_GRID.items():\n",
    "    print(f\"   {param}: {values}\")\n",
    "\n",
    "total_combinations = 1\n",
    "for values in PARAM_GRID.values():\n",
    "    total_combinations *= len(values)\n",
    "print(f\"\\nĞ’ÑĞµĞ³Ğ¾ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¹: {total_combinations}\")\n",
    "print(f\"Ğ­Ğ¿Ğ¾Ñ… Ğ½Ğ° ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ: {SEARCH_EPOCHS}\")\n",
    "\n",
    "# Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¿Ğ¾Ğ¸ÑĞºĞ°\n",
    "from itertools import product\n",
    "import gc\n",
    "\n",
    "results = []\n",
    "best_val_acc = 0\n",
    "best_params = {}\n",
    "\n",
    "param_names = list(PARAM_GRID.keys())\n",
    "param_values = list(PARAM_GRID.values())\n",
    "\n",
    "for i, combo in enumerate(product(*param_values)):\n",
    "    params = dict(zip(param_names, combo))\n",
    "    \n",
    "    print(f\"\\n[{i+1}/{total_combinations}] Testing: {params}\")\n",
    "    \n",
    "    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ ÑÑ‚Ğ¸Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸\n",
    "    test_model = CNNLSTMModel(\n",
    "        n_classes=3,\n",
    "        lookback=LOOKBACK,\n",
    "        conv_filters=params['conv_filters'],\n",
    "        lstm_units=params['lstm_units'],\n",
    "        dropout=params['dropout'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        device='cuda',\n",
    "        random_seed=42\n",
    "    )\n",
    "    \n",
    "    # Ğ‘Ñ‹ÑÑ‚Ñ€Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ\n",
    "    test_model.fit(\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        feature_names=feature_cols,\n",
    "        epochs=SEARCH_EPOCHS,\n",
    "        batch_size=params['batch_size'],\n",
    "        patience=SEARCH_PATIENCE,\n",
    "        use_class_weights=True  # Handle class imbalance (SIDEWAYS ~40%)\n",
    "    )\n",
    "    \n",
    "    # ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ° validation\n",
    "    val_metrics = test_model.evaluate(X_val, y_val)\n",
    "    val_acc = val_metrics['accuracy']\n",
    "    \n",
    "    results.append({**params, 'val_accuracy': val_acc})\n",
    "    print(f\"   Val Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_params = params.copy()\n",
    "        print(f\"   â­ New best!\")\n",
    "    \n",
    "    # ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸\n",
    "    del test_model\n",
    "    gc.collect()\n",
    "\n",
    "# Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ°\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š TUNING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results).sort_values('val_accuracy', ascending=False)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nğŸ† Best parameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"   {param}: {value}\")\n",
    "print(f\"   Val Accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "# Save best params to JSON for train.py to use\n",
    "import json\n",
    "best_params_to_save = {\n",
    "    **best_params,\n",
    "    'val_accuracy': best_val_acc,\n",
    "    'lookback': LOOKBACK  # Include lookback used in grid search\n",
    "}\n",
    "Path('models_artifacts').mkdir(exist_ok=True)\n",
    "with open('models_artifacts/cnn_lstm_best_params.json', 'w') as f:\n",
    "    json.dump(best_params_to_save, f, indent=2)\n",
    "print(f\"\\nâœ… Best params saved: models_artifacts/cnn_lstm_best_params.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782109c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TRAIN FINAL MODEL WITH BEST PARAMETERS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ TRAINING FINAL MODEL WITH BEST PARAMETERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸\n",
    "model = CNNLSTMModel(\n",
    "    n_classes=3,\n",
    "    lookback=LOOKBACK,\n",
    "    conv_filters=best_params.get('conv_filters', 32),\n",
    "    lstm_units=best_params.get('lstm_units', 64),\n",
    "    dropout=best_params.get('dropout', 0.2),\n",
    "    learning_rate=best_params.get('learning_rate', 0.001),\n",
    "    device='cuda',\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Final model architecture:\")\n",
    "print(f\"   Conv filters: {best_params.get('conv_filters', 32)}\")\n",
    "print(f\"   LSTM units: {best_params.get('lstm_units', 64)}\")\n",
    "print(f\"   Dropout: {best_params.get('dropout', 0.2)}\")\n",
    "print(f\"   Learning rate: {best_params.get('learning_rate', 0.001)}\")\n",
    "print(f\"   Batch size: {best_params.get('batch_size', 64)}\")\n",
    "\n",
    "# ĞŸĞ¾Ğ»Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    feature_names=feature_cols,\n",
    "    epochs=100,\n",
    "    batch_size=best_params.get('batch_size', 64),\n",
    "    patience=15,\n",
    "    use_class_weights=True  # Handle class imbalance (SIDEWAYS ~40%)\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340b9002",
   "metadata": {},
   "source": [
    "## 4. Training History Visualization\n",
    "\n",
    "\n",
    "The training curves tell us:\n",
    "- Is the model learning? (loss decreasing)\n",
    "- Is it overfitting? (train loss << val loss)\n",
    "- When did it stop improving? (early stopping point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6122629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PLOT TRAINING HISTORY\n",
    "# ==============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Loss curves\n",
    "# Good sign: Both curves decreasing, staying close together\n",
    "# Bad sign: Train loss much lower than val loss (overfitting)\n",
    "axes[0].plot(model.history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(model.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0].set_ylabel('Loss (Cross-Entropy)', fontsize=11)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy curve\n",
    "# Good sign: Accuracy increasing and stabilizing\n",
    "axes[1].plot(model.history['val_acc'], label='Validation Accuracy', \n",
    "             linewidth=2, color='green')\n",
    "axes[1].axhline(y=0.333, color='red', linestyle='--', label='Random Baseline (33.3%)')\n",
    "axes[1].set_xlabel('Epoch', fontsize=11)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[1].set_title('Validation Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cnn_lstm_training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Analyze training\n",
    "final_train_loss = model.history['train_loss'][-1]\n",
    "final_val_loss = model.history['val_loss'][-1]\n",
    "print(\"\\nğŸ’¡ TRAINING ANALYSIS:\")\n",
    "print(f\"   Final train loss: {final_train_loss:.4f}\")\n",
    "print(f\"   Final val loss:   {final_val_loss:.4f}\")\n",
    "if final_val_loss > final_train_loss * 1.5:\n",
    "    print(\"   âš ï¸ Gap suggests some overfitting - consider more dropout\")\n",
    "else:\n",
    "    print(\"   âœ… Curves are close - model is not overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e5dfab",
   "metadata": {},
   "source": [
    "## 5. Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b51e1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FINAL EVALUATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¯ TEST SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_metrics = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"\\nTest Set Results:\")\n",
    "print(f\"  Accuracy:    {test_metrics['accuracy']:.4f} ({test_metrics['accuracy']:.1%})\")\n",
    "print(f\"  F1 Weighted: {test_metrics['f1_weighted']:.4f}\")\n",
    "print(f\"  F1 Macro:    {test_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "print(f\"\\n  Random Baseline: 33.3%\")\n",
    "print(f\"  Improvement:     {(test_metrics['accuracy'] - 0.333)*100:+.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3daff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CONFUSION MATRIX\n",
    "# ==============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "cm = np.array(test_metrics['confusion_matrix'])\n",
    "labels = ['DOWN', 'SIDEWAYS', 'UP']\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=labels, yticklabels=labels, ax=ax,\n",
    "            annot_kws={\"size\": 14})\n",
    "ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax.set_ylabel('Actual Label', fontsize=12)\n",
    "ax.set_title('Confusion Matrix - CNN-LSTM', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cnn_lstm_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f664a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SAVE MODEL\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ’¾ SAVING MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "Path('models_artifacts').mkdir(exist_ok=True)\n",
    "model.save('models_artifacts', name=f'cnn_lstm_h{HORIZON}')\n",
    "print(f\"âœ… Model saved: models_artifacts/cnn_lstm_h{HORIZON}_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c469a2",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Experiments\n",
    "\n",
    "**WHY EXPERIMENT WITH LOOKBACK?**\n",
    "\n",
    "The lookback window is one of the most important hyperparameters:\n",
    "- Too short: Model can't see enough history\n",
    "- Too long: Model may overfit to noise\n",
    "\n",
    "We test several values to find the optimal one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c70f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LOOKBACK EXPERIMENT\n",
    "# ==============================================================================\n",
    "#\n",
    "# Test different lookback values to find optimal sequence length.\n",
    "# This takes a while as we train multiple models.\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ”¬ LOOKBACK EXPERIMENTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"Testing different lookback values...\")\n",
    "print(\"This will train 4 models and may take 10-15 minutes.\\n\")\n",
    "\n",
    "lookback_values = [5, 10, 20, 30]\n",
    "lookback_results = []\n",
    "\n",
    "for lb in lookback_values:\n",
    "    print(f\"\\n--- Testing lookback={lb} ({lb * 15} minutes of history) ---\")\n",
    "    \n",
    "    model_exp = CNNLSTMModel(\n",
    "        n_classes=3,\n",
    "        lookback=lb,\n",
    "        conv_filters=16,\n",
    "        lstm_units=64,\n",
    "        dropout=0.5,\n",
    "        device='cuda',\n",
    "        random_seed=42\n",
    "    )\n",
    "    \n",
    "    # Train with fewer epochs for experiments\n",
    "    model_exp.fit(\n",
    "        X_train, y_train, \n",
    "        X_val, y_val,\n",
    "        epochs=50,       # Fewer epochs for speed\n",
    "        batch_size=64, \n",
    "        patience=10,\n",
    "        scale=True\n",
    "    )\n",
    "    \n",
    "    metrics = model_exp.evaluate(X_test, y_test)\n",
    "    \n",
    "    lookback_results.append({\n",
    "        'lookback': lb,\n",
    "        'minutes': lb * 15,\n",
    "        'accuracy': metrics['accuracy'],\n",
    "        'f1_weighted': metrics['f1_weighted']\n",
    "    })\n",
    "    \n",
    "    print(f\"   Accuracy: {metrics['accuracy']:.4f}, F1: {metrics['f1_weighted']:.4f}\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š LOOKBACK COMPARISON RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_df = pd.DataFrame(lookback_results)\n",
    "print(\"\\n\" + results_df.to_string(index=False))\n",
    "\n",
    "# Find best\n",
    "best_idx = results_df['accuracy'].idxmax()\n",
    "best_lookback = results_df.loc[best_idx, 'lookback']\n",
    "print(f\"\\nâœ… Best lookback: {best_lookback} candles ({best_lookback * 15} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cbd15b",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7762f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“‹ CNN-LSTM MODEL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "âš™ï¸ CONFIGURATION\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "â€¢ Lookback: {LOOKBACK} candles ({LOOKBACK * 15} minutes)\n",
    "â€¢ Architecture: Conv1D(16) â†’ LSTM(64) â†’ Dense(32) â†’ Output(3)\n",
    "â€¢ Dropout: 50%\n",
    "â€¢ Learning Rate: 0.001\n",
    "â€¢ Optimizer: Adam\n",
    "\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "ğŸ“Š TEST SET RESULTS\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "â€¢ Accuracy:    {test_metrics['accuracy']:.4f} ({test_metrics['accuracy']:.1%})\n",
    "â€¢ F1 Weighted: {test_metrics['f1_weighted']:.4f}\n",
    "â€¢ F1 Macro:    {test_metrics['f1_macro']:.4f}\n",
    "\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "ğŸ”¬ LOOKBACK EXPERIMENT RESULTS\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "{results_df.to_string(index=False)}\n",
    "\n",
    "Best: lookback={best_lookback}\n",
    "\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "ğŸ’¾ SAVED FILES\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "â€¢ Model: models_artifacts/cnn_lstm_h{HORIZON}_model.keras\n",
    "â€¢ Training plot: cnn_lstm_training_history.png\n",
    "â€¢ Confusion matrix: cnn_lstm_confusion_matrix.png\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… CNN-LSTM TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext step: Run 04_comparison.py to compare models.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
